[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "博士期间作业",
    "section": "",
    "text": "Preface\n博一期间课程作业 包括贝叶斯、因果推断、生存分析",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html#readme",
    "href": "index.html#readme",
    "title": "PhD_hw",
    "section": "Readme",
    "text": "Readme\n博一期间课程作业 包括贝叶斯、因果推断、生存分析（有点难）",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "bayesian.html",
    "href": "bayesian.html",
    "title": "(APPENDIX) 附录",
    "section": "",
    "text": "一些定理的证明",
    "crumbs": [
      "(APPENDIX) 附录"
    ]
  },
  {
    "objectID": "survival.html",
    "href": "survival.html",
    "title": "2  Survival",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Survival</span>"
    ]
  },
  {
    "objectID": "whatif.html",
    "href": "whatif.html",
    "title": "3  What if",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>What if</span>"
    ]
  },
  {
    "objectID": "bayesian hw 1.html",
    "href": "bayesian hw 1.html",
    "title": "bayesian hw 1",
    "section": "",
    "text": "EXERCISE 3.1.\nLet prior uncertainty about a parameter \\(\\theta\\) be reflected by the density\n\\[\np(\\theta) = ce^{−3\\theta} I(0,\\infty)(\\theta).\n\\]\nFind the constant c that makes this integrate to one. Also find \\(Pr(\\theta &gt; 2)\\) and \\(Pr(\\theta &gt; 4 | \\theta &gt; 2)\\). Find the median and the expected value. Finally, obtain a 95% probability interval for \\(\\theta\\)\n\n\n\n\\[\\begin{align}\n\n  \\int_{0}^{\\infty} p(\\theta) d\\theta &= \\int_{0}^{\\infty} ce^{-3\\theta}I_{(0,\\infty)}(\\theta) d\\theta \\\\\n\n  &= -\\frac{c}{3}(e^{-3\\theta}|^{\\infty}_0) \\\\\n\n  &= \\frac{c}{3}\n\n\\end{align}\\]\nlet \\(\\frac{c}{3}=1\\)，then we have \\(c=3\\)\n\n\n\n\\[\\begin{align}\n\n  Pr(\\theta &gt; x )&= \\int_{x}^{\\infty} 3e^{-3\\theta}I_{(0,\\infty)}(\\theta) d\\theta \\\\\n\n  &= -e^{-3\\theta}|^{\\infty}_x \\\\\n\n  &= e^{-3\\theta}|^{x}_\\infty  \\\\\n  \n  &= e^{-3x}\n\n\\end{align}\\]\nlet \\(x=2\\), then we have \\[\nPr(\\theta&gt;2) = e^{-3*2} = 0.002478752\n\\]\n\\[\nPr(\\theta &gt; 4 | \\theta &gt; 2) = Pr(\\theta&gt;2) = e^{-3*2} = 0.002478752\n\\]\n\n# Pr(theta&gt;2)\n1-pexp(2,3)\n\n[1] 0.002478752\n\n# Monte Carlo simulations\nset.seed(2024)\nN &lt;- 1000000 \nsamples &lt;- rexp(N,3)\nP &lt;- mean(samples &gt; 2)\nprint(P)\n\n[1] 0.002496\n\n\n\n# Pr(theta&gt;4|theta&gt;2)\n(1-pexp(4,3))/(1-pexp(2,3))\n\n[1] 0.002478752\n\n\n\nFor Exp(3) , \\(Mean = \\frac{1}{\\theta}\\) ,\\(Mdian = \\frac{Ln2}{\\theta}\\), We all know \\(\\theta =3\\), so we have \\(Mean = \\frac{1}{3}\\) , \\(Median = \\frac{Ln2}{3}\\)\n95% probability interval for \\(\\theta\\)\n\n\nqexp(c(0.025,0.975),3)\n\n[1] 0.008439269 1.229626485\n\n\n\n\nEXERCISE 3.2.\nSuppose \\(n\\) cities were sampled and for each city \\(i\\) the number \\(y_i\\) of deaths from ALS were recorded for a period of one year. We expect the numbers to be Poisson distributed, but the size of the city is a factor. Let \\(M_i\\) be the known population for city \\(i\\) and let\n\\[\ny_i|\\theta ～ Pois(\\theta M_i), i= 1,...k\n\\]\nwhere \\(\\theta\\) &gt; 0 is an unknown parameter measuring the common death rate for all cities. Given \\(\\theta\\), the expected number of ALS deaths for city i is \\(\\theta M_i\\), so \\(\\theta\\) is expected to be small. Assume that independent scientific information can be obtained about \\(\\theta\\) in the form of a gamma distribution, say \\(Gamma(a, b)\\). Show that this prior and posterior are conjugate in the sense that both have gamma distributions.\n\n\n\n\\[\n\\theta ～ Gamma(a,b)  \\Rightarrow  p(\\theta) = [b^a/\\Gamma(a)]\\theta^{a-1}e^{-b\\theta}I_{(0,\\infty)}(\\theta)\n\\]\n\\[\ny_i|\\theta～Pois(\\theta M_i)\\Rightarrow P(y_i|\\theta) = \\frac{(\\theta M_i)^{Y_i}e^{-\\theta M_i}}{Y_i!}\n\\]\n\\[\nL(\\theta)= (\\prod \\frac{M_{i}^{y_i}}{y_i !}) \\theta^{\\sum y_i} e^{-\\theta \\sum M_i }\n\\]\n\\[\\begin{align}\n\n  p(\\theta|y) & \\propto p(\\theta)L(\\theta) \\\\\n\n  & \\propto  \\theta^{a-1}e^{-b\\theta}\\theta^{\\sum y_i} e^{- \\theta \\sum M_i} \\\\\n\n  & \\propto \\theta^{a+\\sum y_i -1} e^{- \\theta (b+\\sum M_i)}\n\n\\end{align}\\]\nFinally, we have\n\\[\n\\theta|y ～ Gamma(a+\\sum y_i  ,b+\\sum M_i)\n\\]\n\n\nEXERCISE 3.3.\nExtending Exercise 3.2, two cities are allowed different death rates. Let \\(y_i ～  Pois(θ_i M_i), i = 1, 2,\\) where the \\(M_is\\) are known constants. Let knowledge about \\(θ_i\\) be reflected by independent gamma distributions, namely \\(θ_i ∼ Gamma(a_i, b_i)\\). Derive the joint posterior for \\((\\theta_1,\\theta_2)\\). Characterize the joint distribution as we did for sampling two independent binomials. Think of \\(\\theta_i\\) as the rate of events per 100 thousand people in city \\(i\\). For independent priors \\(\\theta_i ∼ Gamma(1, 0.1)\\), give the exact joint posterior with \\(y_1 = 500, y_2 = 800\\)in cities with populations of 100 thousand and 200 thousand, respectively.\n\n\n\n\\[\nP(\\theta_1 , \\theta_2)=p_1(\\theta_1)p_2(\\theta_2)\n\\]\nAccording to the above conclusion, we have \\[\n\\theta_1,\\theta_2 ～ Gamma(a + y_1,b+M_1)Gamma(a + y_2,b+M_2)\n\\]\nWe know,\n\\[\\begin{align}\n\na_1 = a_2 &= 1 \\\\\n\nb_1 = b_2 &= 0.1 \\\\\n\ny_1 &= 500  \\\\\n\ny_2 &= 800  \\\\\n\nM_1 &= 100  \\\\\n\nM_2 &= 200\n\n\\end{align}\\]\nFinally,\n\\[\\begin{align}\np(\\theta_1,\\theta_2|y_1 =500,y_2=800)=[[100.1^501/\\Gamma(501)]\\theta_1 ^{500}e^{-100.1\\theta_1}][[200.1^801/\\Gamma(801)]\\theta_2 ^{800-1}e^{-200.1\\theta_2}]\n\n\n\\end{align}\\]\n\n\nEXERCISE 3.4.\nPerform Example 3.1.3 in WinBUGS with \\(y_1 ∼ Bin(80,\\theta1)\\), \\(y_2 ∼ Bin(100,\\theta_2)\\) \\(\\theta1 ∼ Beta(1, 1), \\theta 2 ∼ Beta(2, 1)\\) with observations \\(y_1 = 32\\) and \\(y_2 = 35\\). Put each term in the model on a separate line. There should still be only two list statements with entries separated by commas. See Exercises 3.6 and 3.7 for WinBUGS syntax.\n\nsuppressMessages(library(rjags))\n\nmodel_string &lt;- \"\nmodel {\n\n    theta1 ~ dbeta(1, 1)\n    theta2 ~ dbeta(2, 1)\n    \n    y1 ~ dbin(theta1, n1)\n    y2 ~ dbin(theta2, n2)\n    \n}\n\"\n\ndata_list &lt;- list(\n    y1 = 32,\n    n1 = 80,\n    y2 = 35,\n    n2 = 100\n)\n\n\njags_model &lt;- jags.model(\n    textConnection(model_string),\n    data = data_list,\n    n.chains = 1,\n    n.adapt = 1000\n)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 2\n   Unobserved stochastic nodes: 2\n   Total graph size: 8\n\nInitializing model\n\nupdate(jags_model, 1000)    \n\nmcmc_samples &lt;- coda.samples(\n    model = jags_model,\n    variable.names = c(\"theta1\", \"theta2\", \"gamma\"),\n    n.iter = 10000\n)\n\nWarning in FUN(X[[i]], ...): Failed to set trace monitor for gamma\nVariable gamma not found\n\nhead(mcmc_samples[[1]])\n\nMarkov Chain Monte Carlo (MCMC) output:\nStart = 2001 \nEnd = 2007 \nThinning interval = 1 \n        theta1    theta2\n[1,] 0.5082423 0.3153096\n[2,] 0.4995938 0.3589142\n[3,] 0.4962379 0.3091079\n[4,] 0.4325777 0.3250655\n[5,] 0.4181803 0.3353077\n[6,] 0.3788849 0.4116635\n[7,] 0.3470925 0.3698580\n\n\n\n\nEXERCISE 3.5.\nPerform a data analysis for the model in Exercise 3.3 using the data \\(y_1 = 500\\), \\(y_2 = 800\\), \\(M_1 = 100\\), \\(M_2 = 200\\), and using independent \\(Gamma(1, 0.01)\\) priors for the \\(θ_is\\). Make WinBUGS based inferences for all parameters and functions of parameters discussed there using a Monte Carlo sample size of 10,000 and a burn-in of 1,000. This may involve an excursion into the “Help” menu to find the syntax for Poisson and gamma distributions. Compare the posterior means for \\(\\theta_1\\) and \\(\\theta_2\\) based on the WinBUGS output to the exact values from the Gamma posteriors that you obtained in Exercise 3.3.\n\nmodel_string &lt;- \"\nmodel {\n    theta1 ~ dgamma(a, b)\n    theta2 ~ dgamma(a, b)\n    \n    y1 ~ dpois(theta1*M1)\n    y2 ~ dpois(theta2*M2)\n}\n\"\n\ndata_list &lt;- list(\n    a = 1,\n    b = 0.01,\n    y1 = 500,\n    M1 = 100,\n    y2 = 800,\n    M2 = 200\n)\n\njags_model &lt;- jags.model(\n    textConnection(model_string),\n    data = data_list,\n    n.chains = 1,\n    n.adapt = 10000\n)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 2\n   Unobserved stochastic nodes: 2\n   Total graph size: 10\n\nInitializing model\n\nupdate(jags_model, 1000)    \n\nmcmc_samples &lt;- coda.samples(\n    model = jags_model,\n    variable.names = c(\"theta1\", \"theta2\"),\n    n.iter = 10000\n)\n\nhead(mcmc_samples[[1]])\n\nMarkov Chain Monte Carlo (MCMC) output:\nStart = 1001 \nEnd = 1007 \nThinning interval = 1 \n       theta1   theta2\n[1,] 5.068278 4.187043\n[2,] 5.484848 3.970771\n[3,] 5.158063 3.949658\n[4,] 4.720652 3.929965\n[5,] 4.692881 4.068471\n[6,] 4.858593 4.245358\n[7,] 5.125394 4.188519\n\napply(mcmc_samples[[1]],2,mean)\n\n  theta1   theta2 \n5.007827 4.007279 \n\n# gamma分布均值为a/b\n\n501/100.1\n\n[1] 5.004995\n\n801/200.1\n\n[1] 4.002999\n\n\n\n\nEXERCISE 3.6.\n\nmodel_string &lt;- \"\nmodel {\n    y ~ dbin(theta , n)\n    ytilde ~ dbin(theta, m)\n    theta ~ dbeta(a, b)\n    prob &lt;- step(ytilde - 20)\n}\n\"\n\ndata_list &lt;- list(n=100, m=100, y=10, a=1, b=1)\n\njags_model &lt;- jags.model(\n    textConnection(model_string),\n    data = data_list,\n    n.chains = 1,\n    n.adapt = 1000\n)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 1\n   Unobserved stochastic nodes: 2\n   Total graph size: 10\n\nInitializing model\n\nupdate(jags_model, 100)    \n\nmcmc_samples &lt;- coda.samples(\n    model = jags_model,\n    variable.names = c(\"theta\",'prob'),\n    n.iter = 10000\n)\n\napply(mcmc_samples[[1]],2,mean)\n\n     prob     theta \n0.0341000 0.1073693 \n\n\n\n\nEXERCISE 3.7.\n\nmodel_string &lt;- \"\nmodel{ \n  y1 ~ dbin(theta1, n1) \n  y2 ~ dbin(theta2, n2) \n  theta1 ~ dbeta(1, 1) \n  theta2 ~ dbeta(1, 1) \n  prob1 &lt;- step(theta1-theta2) \n  y1tilde ~ dbin(theta1, m1) \n  y2tilde ~ dbin(theta2, m2) \n  prob2 &lt;- step(y1tilde - y2tilde - 11) \n  gamma &lt;- theta1-theta2\n} \n\"\ndata_list &lt;- list(y1=25, y2=10, n1=100, n2=100, m1=100, m2=100) \n\njags_model &lt;- jags.model(\n    textConnection(model_string),\n    data = data_list,\n    n.chains = 1,\n    n.adapt = 10000\n)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 2\n   Unobserved stochastic nodes: 4\n   Total graph size: 17\n\nInitializing model\n\nupdate(jags_model, 5000)    \n\nmcmc_samples &lt;- coda.samples(\n    model = jags_model,\n    variable.names = c(\"theta1\",\"theta2\",'prob1','prob2','gamma','y1tilde','y2tilde'),\n    n.iter = 10000\n)\napply(mcmc_samples[[1]],2,mean)\n\n     gamma      prob1      prob2     theta1     theta2    y1tilde    y2tilde \n 0.1466940  0.9964000  0.7109000  0.2543524  0.1076584 25.4371000 10.8075000 \n\nquantile(mcmc_samples[[1]][,'gamma'],c(0.025,0.975))\n\n      2.5%      97.5% \n0.04619207 0.25089176",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 1"
    ]
  },
  {
    "objectID": "survival hw 1.html",
    "href": "survival hw 1.html",
    "title": "survival hw 1",
    "section": "",
    "text": "KM 8.1\nIn section 1.10, times to death or relapse (in days) are given for 23 nonHodgkin’s lymphoma (NHL) patients, 11 receiving an allogenic (Allo) transplant from an HLA-matched sibling donor and 12 patients receiving an autologous (Auto) transplant. Also, data on 20 Hodgkin’s lymphoma (HOD) patients, 5 receiving an allogenic (Allo) transplant from an HLAmatched sibling donor and 15 patients receiving an autologous (Auto) transplant is given.\n(a) Treating NHL Allo as the baseline hazard function, state the appropriate coding which would allow the investigator to test for any difference in survival functions for the four groups, treating them as four independent groups.\nrm(list = ls())\npkgs &lt;- c('tidyverse','skimr','survival','KMsurv')\ninvisible(lapply(pkgs, function(x) suppressMessages(library(x,character.only = TRUE)) ))\n\ndata(hodg)\nskim(hodg)\n\n\nData summary\n\n\nName\nhodg\n\n\nNumber of rows\n43\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ngtype\n0\n1\n1.63\n0.49\n1\n1\n2\n2.0\n2\n▅▁▁▁▇\n\n\ndtype\n0\n1\n1.47\n0.50\n1\n1\n1\n2.0\n2\n▇▁▁▁▇\n\n\ntime\n0\n1\n429.74\n570.54\n2\n55\n132\n504.0\n2144\n▇▂▁▁▁\n\n\ndelta\n0\n1\n0.60\n0.49\n0\n0\n1\n1.0\n1\n▅▁▁▁▇\n\n\nscore\n0\n1\n76.28\n21.05\n20\n60\n80\n90.0\n100\n▁▂▂▃▇\n\n\nwtime\n0\n1\n37.70\n33.62\n5\n16\n24\n55.5\n171\n▇▃▂▁▁\n\n\n\n\nhodg$new &lt;- ifelse(hodg$gtype==1 & hodg$dtype==1,'allo_nhl',\n                            ifelse(hodg$gtype==2 & hodg$dtype==1,'autlo_nhl',ifelse(hodg$gtype==1 & hodg$dtype==2,'allo_hod','auto_hod'))) %&gt;% factor() %&gt;% relevel(.,ref = \"allo_nhl\")\n\nmodel &lt;- coxph(Surv(time,delta)~new,data=hodg,ties = 'breslow')\nsummary(model)\n\nCall:\ncoxph(formula = Surv(time, delta) ~ new, data = hodg, ties = \"breslow\")\n\n  n= 43, number of events= 26 \n\n               coef exp(coef) se(coef)     z Pr(&gt;|z|)   \nnewallo_hod  1.8297    6.2323   0.6753 2.709  0.00674 **\nnewautlo_nhl 0.6639    1.9423   0.5643 1.177  0.23939   \nnewauto_hod  0.1537    1.1662   0.5888 0.261  0.79406   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n             exp(coef) exp(-coef) lower .95 upper .95\nnewallo_hod      6.232     0.1605    1.6589    23.414\nnewautlo_nhl     1.942     0.5149    0.6427     5.870\nnewauto_hod      1.166     0.8575    0.3677     3.698\n\nConcordance= 0.605  (se = 0.061 )\nLikelihood ratio test= 7.89  on 3 df,   p=0.05\nWald test            = 9.26  on 3 df,   p=0.03\nScore (logrank) test = 11.08  on 3 df,   p=0.01\n(b) Treating NHL Allo as the baseline hazard function, state the appropriate coding which would allow the investigator to test for an interaction between type of transplant and disease type using main effects and interaction terms.\nmodel &lt;- coxph(Surv(time,delta)~gtype * dtype,data=hodg,ties = 'breslow')\nsummary(model)\n\nCall:\ncoxph(formula = Surv(time, delta) ~ gtype * dtype, data = hodg, \n    ties = \"breslow\")\n\n  n= 43, number of events= 26 \n\n                coef exp(coef) se(coef)      z Pr(&gt;|z|)   \ngtype        3.00377  20.16134  1.30504  2.302  0.02135 * \ndtype        4.16964  64.69233  1.45172  2.872  0.00408 **\ngtype:dtype -2.33990   0.09634  0.85168 -2.747  0.00601 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\ngtype        20.16134    0.04960   1.56196  260.2361\ndtype        64.69233    0.01546   3.75963 1113.1660\ngtype:dtype   0.09634   10.38019   0.01815    0.5114\n\nConcordance= 0.605  (se = 0.061 )\nLikelihood ratio test= 7.89  on 3 df,   p=0.05\nWald test            = 9.26  on 3 df,   p=0.03\nScore (logrank) test = 11.08  on 3 df,   p=0.01\n(c) Suppose that we have the following model for the hazard rates in the four groups:\nWhat are the risk coefficients, \\(\\beta_i\\) , i = 1, 2, 3, for the interaction model in part b ?\n\\[\nh(t)=h_0(t)exp(\\beta_1Auto + \\beta_2 HOD + \\beta3 Auto*HOD)\n\\] allo_nhl 0 0\nallo_hod 0 1\nnhl_auto 1 0\nhod__auto 1 1\n\\[\\begin{align}\n\\beta_1 & = 1.5 \\\\\n\\beta_2 &= 2 \\\\\n\\beta_1 + beta_2 +\\beta_3 &= 0.5 \\\\\n\\beta_3 &= 0.5-1.5-2 =-3\n\\end{align}\\]",
    "crumbs": [
      "Survival",
      "survival hw 1"
    ]
  },
  {
    "objectID": "causal inference hw 1.html",
    "href": "causal inference hw 1.html",
    "title": "causal inference hw 1",
    "section": "",
    "text": "作业内容：\n作业1： 将B书6.1.2节（Standardization with a parametric outcome model）的2个案例代码翻译为计算公式，需标明代码与公式的对应关系。 截止时间：10月24日晚上10点\n作业2： 查看10.24“需阅读材料”，提前阅读文献，准备当次课讲解文献（无需做ppt）。\nB书《Fundamentals of Causal Inference With R (Babette A. Brumback) 》\n数据:\nhttps://s3-eu-west-1.amazonaws.com/s3-euw1-ap-pe-ws4-cws-documents.ri-prod/9780367705053/Brumback_FOCI_Website_Material.zip",
    "crumbs": [
      "Causal inference",
      "causal inference hw 1"
    ]
  },
  {
    "objectID": "Bayesian.html",
    "href": "Bayesian.html",
    "title": "PhD_hw",
    "section": "",
    "text": "book: - part: “Bayesian” chapters: - basics.qmd - packages.qmd",
    "crumbs": [
      "Bayesian.html"
    ]
  },
  {
    "objectID": "Linear regression.html",
    "href": "Linear regression.html",
    "title": "Linear regression",
    "section": "",
    "text": "线性建模大纲：\nhttps://docs.qq.com/doc/DUG1ZRnJOeGJwUHli",
    "crumbs": [
      "Linear regression"
    ]
  },
  {
    "objectID": "linear regression hw 3.html",
    "href": "linear regression hw 3.html",
    "title": "linear regression hw 3",
    "section": "",
    "text": "作业内容：\nLinear Models with R (2nd Edition)第2章所有题",
    "crumbs": [
      "Linear regression",
      "linear regression hw 3"
    ]
  },
  {
    "objectID": "linear regression hw 3.html#作业内容",
    "href": "linear regression hw 3.html#作业内容",
    "title": "linear regression hw 3",
    "section": "",
    "text": "1",
    "crumbs": [
      "Linear regression",
      "linear regression hw 3"
    ]
  },
  {
    "objectID": "linear regression hw 3.html#for-the-prostate-data-fit-a-model-with-lpsa-as-the-response-and-the-other-variables-as-predictors-a-compute-90-and-95-cis-for-the-parameter-associated-with-age.-using-just",
    "href": "linear regression hw 3.html#for-the-prostate-data-fit-a-model-with-lpsa-as-the-response-and-the-other-variables-as-predictors-a-compute-90-and-95-cis-for-the-parameter-associated-with-age.-using-just",
    "title": "linear regression hw 3",
    "section": "1. For the prostate data, fit a model with lpsa as the response and the other variables as predictors: (a) Compute 90 and 95% CIs for the parameter associated with age. Using just",
    "text": "1. For the prostate data, fit a model with lpsa as the response and the other variables as predictors: (a) Compute 90 and 95% CIs for the parameter associated with age. Using just",
    "crumbs": [
      "Linear regression",
      "linear regression hw 3"
    ]
  },
  {
    "objectID": "linear regression hw 3.html#section",
    "href": "linear regression hw 3.html#section",
    "title": "linear regression hw 3",
    "section": "1.",
    "text": "1.\nFor the prostate data, fit a model with lpsa as the response and the other variables as predictors: (a) Compute 90 and 95% CIs for the parameter associated with age. Using just",
    "crumbs": [
      "Linear regression",
      "linear regression hw 3"
    ]
  },
  {
    "objectID": "linear regression hw 3.html#exercise-1",
    "href": "linear regression hw 3.html#exercise-1",
    "title": "linear regression hw 3",
    "section": "Exercise 1",
    "text": "Exercise 1\nThe dataset teengamb concerns a study of teenage gambling in Britain. Fit a regression model with the expenditure on gambling as the response and the sex, status, income and verbal score as predictors. Present the output.\n\nrm(list=ls())\n\npkgs &lt;- c('tidyverse','faraway','skimr','ggplot2')\ninvisible(lapply(pkgs, function(x) suppressMessages(library(x,character.only = TRUE))))\n\ndata(teengamb)\nglimpse(teengamb)\n\nRows: 47\nColumns: 5\n$ sex    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, …\n$ status &lt;int&gt; 51, 28, 37, 28, 65, 61, 28, 27, 43, 18, 18, 43, 30, 28, 38, 38,…\n$ income &lt;dbl&gt; 2.00, 2.50, 2.00, 7.00, 2.00, 3.47, 5.50, 6.42, 2.00, 6.00, 3.0…\n$ verbal &lt;int&gt; 8, 8, 6, 4, 8, 6, 7, 5, 6, 7, 6, 6, 4, 6, 6, 8, 8, 5, 8, 9, 8, …\n$ gamble &lt;dbl&gt; 0.00, 0.00, 0.00, 7.30, 19.60, 0.10, 1.45, 6.60, 1.70, 0.10, 0.…\n\n\n(a) What percentage of variation in the response is explained by these predictors?\n\nmodel &lt;- lm(gamble ~ ., data = teengamb )\nmodelsum &lt;- summary(model)\nmodelsum$r.squared\n\n[1] 0.5267234\n\n\n(b) Which observation has the largest (positive) residual? Give the case number.\n\nnum &lt;- unname(which.max(modelsum$residuals))\nnum\n\n[1] 24\n\n\n(c) Compute the mean and median of the residuals.\n\nmean(modelsum$residuals)\n\n[1] 2.645638e-16\n\nmedian(modelsum$residuals)\n\n[1] -1.451392\n\n\n(d) Compute the correlation of the residuals with the fitted values.\n\nplot(modelsum$residuals,predict(model))\n\n\n\n\n\n\n\ncor(modelsum$residuals,predict(model))\n\n[1] -1.030987e-16\n\n\n(e) Compute the correlation of the residuals with the income.\n\ncor(modelsum$residuals,teengamb$income)\n\n[1] -2.006086e-17\n\n\n(f) For all other predictors held constant, what would be the difference in predicted expenditure on gambling for a male compared to a female?\n\nsummary(model)$coefficients['sex','Estimate']\n\n[1] -22.11833",
    "crumbs": [
      "Linear regression",
      "linear regression hw 3"
    ]
  },
  {
    "objectID": "linear regression hw 3.html#exercise-2",
    "href": "linear regression hw 3.html#exercise-2",
    "title": "linear regression hw 3",
    "section": "Exercise 2",
    "text": "Exercise 2\nThe dataset uswages is drawn as a sample from the Current Population Survey in 1988. Fit a model with weekly wages as the response and years of education and experience as predictors. Report and give a simple interpretation to the regression coefficient for years of education. Now fit the same model but with logged weekly wages. Give an interpretation to the regression coefficient for years of education. Which interpretation is more natural?\n\ndata('uswages')\nglimpse(uswages)\n\nRows: 2,000\nColumns: 10\n$ wage  &lt;dbl&gt; 771.60, 617.28, 957.83, 617.28, 902.18, 299.15, 541.31, 148.39, …\n$ educ  &lt;int&gt; 18, 15, 16, 12, 14, 12, 16, 16, 12, 12, 9, 14, 17, 14, 14, 10, 1…\n$ exper &lt;int&gt; 18, 20, 9, 24, 12, 33, 42, 0, 36, 37, 20, 29, 16, 21, 11, 10, 8,…\n$ race  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ smsa  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1…\n$ ne    &lt;int&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n$ mw    &lt;int&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ so    &lt;int&gt; 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1…\n$ we    &lt;int&gt; 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0…\n$ pt    &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n\nhist(uswages$wage)\n\n\n\n\n\n\n\n# GGally::ggpairs(uswages[,c(1:3)])\nmodel &lt;- lm(wage ~ educ + exper, uswages)\nsummary(model)$coefficients['educ','Estimate']\n\n[1] 51.17527\n\n\n\nmodel &lt;- lm(log(wage) ~ educ + exper, uswages)\n#summary(model)$coefficients['educ','Estimate']\nexp(summary(model)$coefficients['educ','Estimate'])\n\n[1] 1.094728\n\n\nY偏态分布，应该log转换后在拟合模型，注意log转化后要转化回来再解释，此外该模型的R-squard很小，模型意义不大",
    "crumbs": [
      "Linear regression",
      "linear regression hw 3"
    ]
  },
  {
    "objectID": "linear regression hw 3.html#exercise-3",
    "href": "linear regression hw 3.html#exercise-3",
    "title": "linear regression hw 3",
    "section": "Exercise 3",
    "text": "Exercise 3\nIn this question, we investigate the relative merits of methods for computing the coefficients. Generate some artificial data by:\n&gt; x &lt;- 1:20 \n&gt; y &lt;- x+rnorm(20)\nFit a polynomial in x for predicting y. Compute \\(\\bar \\beta\\) in two ways — by lm() and by using the direct calculation described in the chapter. At what degree of polynomial does the direct calculation method fail? (Note the need for the I() function in fitting the polynomial, that is, lm(y ~ x + I(x^2))\n\n# lm()法\nx &lt;- 1:20\ny &lt;- x + rnorm(20)\nm &lt;- lm(y ~ x + I(x^2))\nsummary(m)\n\n\nCall:\nlm(formula = y ~ x + I(x^2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1225 -0.5200 -0.2275  0.6088  1.7176 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.731908   0.738251  -0.991    0.335    \nx            1.184552   0.161910   7.316 1.21e-06 ***\nI(x^2)      -0.006675   0.007489  -0.891    0.385    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9923 on 17 degrees of freedom\nMultiple R-squared:  0.9775,    Adjusted R-squared:  0.9748 \nF-statistic: 368.7 on 2 and 17 DF,  p-value: 9.979e-15\n\n\n\n# 直接计算法\nx &lt;- model.matrix(~ x + I(x^2))\nsolve(crossprod(x,x), crossprod(x, y))\n\n                    [,1]\n(Intercept) -0.731907764\nx            1.184552428\nI(x^2)      -0.006675383\n\n\n\n# 循环\n# 计算函数\n\nf &lt;- function(z){\nx &lt;- 1:20\ny &lt;- x + rnorm(20)\nx &lt;- model.matrix(~ x + I(x^z))\nsolve(crossprod(x,x), crossprod(x, y))\n}\n\n #map(2:10,function(z) f(z))",
    "crumbs": [
      "Linear regression",
      "linear regression hw 3"
    ]
  },
  {
    "objectID": "linear regression hw 3.html#exercise-4",
    "href": "linear regression hw 3.html#exercise-4",
    "title": "linear regression hw 3",
    "section": "Exercise 4",
    "text": "Exercise 4\nThe dataset prostate comes from a study on 97 men with prostate cancer who were due to receive a radical prostatectomy. Fit a model with lpsa as the response and lcavol as the predictor. Record the residual standard error and the R2. Now add lweight, svi, lbph, age, lcp, pgg45 and gleason to the model one at a time. For each model record the residual standard error and the R2. Plot the trends in these two statistics.\n\ndata(\"prostate\")\n\nmodel &lt;- lm(lpsa ~ lcavol, prostate)\n# residual standard error\nsummary(model)$sigma\n\n[1] 0.7874994\n\n# R2\nsummary(model)$r.squared\n\n[1] 0.5394319\n\nadd_variable &lt;- c(\"lcavol\",\"lweight\", \"svi\", \"lbph\", \"age\", \"lcp\", \"pgg45\", \"gleason\")\n\nresults &lt;- data.frame(sigma=NULL,r2=NULL,formula=NULL)\nfor(i in 1:length(add_variable)){\n  \nif (i == 1) {\n    model_formula &lt;- paste(\"lpsa ~\", add_variable[i])\n  } else {\n    model_formula &lt;- paste(model_formula, \"+\", add_variable[i])\n  }\n\n  summodel &lt;- summary(lm(formula(model_formula), data = prostate))\n\n  result &lt;- data.frame(sigma=summodel$sigma,r2=summodel$r.squared,formula=model_formula)\n  results &lt;- rbind(results,result)\n}\nprint(results)\n\n      sigma        r2\n1 0.7874994 0.5394319\n2 0.7506469 0.5859345\n3 0.7168094 0.6264403\n4 0.7108232 0.6366035\n5 0.7073054 0.6441024\n6 0.7102135 0.6451130\n7 0.7047533 0.6544317\n8 0.7084155 0.6547541\n                                                             formula\n1                                                      lpsa ~ lcavol\n2                                            lpsa ~ lcavol + lweight\n3                                      lpsa ~ lcavol + lweight + svi\n4                               lpsa ~ lcavol + lweight + svi + lbph\n5                         lpsa ~ lcavol + lweight + svi + lbph + age\n6                   lpsa ~ lcavol + lweight + svi + lbph + age + lcp\n7           lpsa ~ lcavol + lweight + svi + lbph + age + lcp + pgg45\n8 lpsa ~ lcavol + lweight + svi + lbph + age + lcp + pgg45 + gleason\n\n\n\n# Create a plot for sigma\nplot(1:8, results$sigma, type = 'l', col = 'blue', lwd = 2,\n     ylim = c(0.6, 0.9),  # Set the y-axis limits here\n     xlab = 'Model Number', ylab = 'Value',\n     main = 'Residual Standard Error and R-squared')\n\npoints(1:8, results$sigma, col = 'blue', pch = 19)\n\nr2_normalized &lt;- results$r2 * (max(results$sigma) / max(results$r2))\n\nlines(1:8, r2_normalized, col = 'red', lwd = 2)\n\npoints(1:8, r2_normalized, col = 'red', pch = 19)\n\nlegend('topright', legend = c('Residual Standard Error', 'R-squared (scaled)'),\n       col = c('blue', 'red'), lty = 1, pch = 19)",
    "crumbs": [
      "Linear regression",
      "linear regression hw 3"
    ]
  },
  {
    "objectID": "linear regression hw 3.html#exercise-5",
    "href": "linear regression hw 3.html#exercise-5",
    "title": "linear regression hw 3",
    "section": "Exercise 5",
    "text": "Exercise 5\nUsing the prostate data, plot lpsa against lcavol. Fit the regressions of lpsa on lcavol and lcavol on lpsa. Display both regression lines on the plot. At what point do the two lines intersect?\n\nmodel1 &lt;- lm(lcavol ~ lpsa, prostate)\nmodel2 &lt;- lm(lpsa ~ lcavol, prostate)\n\n# -0.50858+0.74992*(1.50730+0.71932*x)=x\n# -0.50858+0.74992*1.50730+0.74992*0.71932*x=x\nx &lt;- (-0.50858+0.74992*1.50730)/(1-0.74992*0.71932)\ny &lt;- 1.50730+0.71932*x\n\nggplot(prostate, aes(lcavol, lpsa)) +\n  geom_point(alpha = .5) +\n  geom_line(aes(x = predict(model1), color = \"lcavol ~ lpsa\")) +\n  geom_line(aes(y = predict(model2), color = \"lpsa ~ lcavol\")) +\n  geom_point(aes(y =y, x = x), shape = 1, size = 5)\n\nWarning in geom_point(aes(y = y, x = x), shape = 1, size = 5): All aesthetics have length 1, but the data has 97 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.",
    "crumbs": [
      "Linear regression",
      "linear regression hw 3"
    ]
  },
  {
    "objectID": "linear regression hw 3.html#exercise-6",
    "href": "linear regression hw 3.html#exercise-6",
    "title": "linear regression hw 3",
    "section": "Exercise 6",
    "text": "Exercise 6\nThirty samples of cheddar cheese were analyzed for their content of acetic acid, hydrogen sulfide and lactic acid. Each sample was tasted and scored by a panel of judges and the average taste score produced. Use the cheddar data to answer the following:\n(a) Fit a regression model with taste as the response and the three chemical contents as predictors. Report the values of the regression coefficients.\n\ndata(\"cheddar\")\nmodel &lt;- lm(taste ~ ., cheddar)\nsumary(model)\n\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -28.87677   19.73542 -1.4632 0.155399\nAcetic        0.32774    4.45976  0.0735 0.941980\nH2S           3.91184    1.24843  3.1334 0.004247\nLactic       19.67054    8.62905  2.2796 0.031079\n\nn = 30, p = 4, Residual SE = 10.13071, R-Squared = 0.65\n\n\n(b) Compute the correlation between the fitted values and the response. Square it. Identify where this value appears in the regression output.\n\ncor(model$fitted.values, cheddar$taste)^2\n\n[1] 0.6517747\n\n\n(c) Fit the same regression model but without an intercept term. What is the value of R2 reported in the output? Compute a more reasonable measure of the goodness of fit for this example.\n\nmodel &lt;- lm(taste ~ -1+., cheddar)\nsummary(model)$r.squared\n\n[1] 0.8877059\n\n#cor(model$fitted.values, cheddar$taste)^2\n\n(d) Compute the regression coefficients from the original fit using the QR decomposition showing your R code.\n\nmodel &lt;- lm(taste ~ ., cheddar)\nm_mat &lt;- model.matrix(model)\n\nqr_decomp &lt;- qr(m_mat)\n\nbacksolve(\n  qr.R(qr_decomp), # upper-right\n  t(qr.Q(qr_decomp)) %*% cheddar$taste\n)\n\n            [,1]\n[1,] -28.8767696\n[2,]   0.3277413\n[3,]   3.9118411\n[4,]  19.6705434",
    "crumbs": [
      "Linear regression",
      "linear regression hw 3"
    ]
  },
  {
    "objectID": "linear regression hw 3.html#exercise-7",
    "href": "linear regression hw 3.html#exercise-7",
    "title": "linear regression hw 3",
    "section": "Exercise 7",
    "text": "Exercise 7\nAn experiment was conducted to determine the effect of four factors on the resistivity of a semiconductor wafer. The data is found in wafer where each of the four factors is coded as − or + depending on whether the low or the high setting for that factor was used. Fit the linear model resist ∼ x1 + x2 + x3 + x4.\n\nExtract the X matrix using the model.matrix function. Examine this to determine how the low and high levels have been coded in the model.\n\n\ndata(\"wafer\")\nglimpse(wafer)\n\nRows: 16\nColumns: 5\n$ x1     &lt;fct&gt; -, +, -, +, -, +, -, +, -, +, -, +, -, +, -, +\n$ x2     &lt;fct&gt; -, -, +, +, -, -, +, +, -, -, +, +, -, -, +, +\n$ x3     &lt;fct&gt; -, -, -, -, +, +, +, +, -, -, -, -, +, +, +, +\n$ x4     &lt;fct&gt; -, -, -, -, -, -, -, -, +, +, +, +, +, +, +, +\n$ resist &lt;dbl&gt; 193.4, 247.6, 168.2, 205.0, 303.4, 339.9, 226.3, 208.3, 220.0, …\n\nmodel &lt;- lm(resist ~ ., wafer)\nx &lt;- model.matrix(model)\nx\n\n   (Intercept) x1+ x2+ x3+ x4+\n1            1   0   0   0   0\n2            1   1   0   0   0\n3            1   0   1   0   0\n4            1   1   1   0   0\n5            1   0   0   1   0\n6            1   1   0   1   0\n7            1   0   1   1   0\n8            1   1   1   1   0\n9            1   0   0   0   1\n10           1   1   0   0   1\n11           1   0   1   0   1\n12           1   1   1   0   1\n13           1   0   0   1   1\n14           1   1   0   1   1\n15           1   0   1   1   1\n16           1   1   1   1   1\nattr(,\"assign\")\n[1] 0 1 2 3 4\nattr(,\"contrasts\")\nattr(,\"contrasts\")$x1\n[1] \"contr.treatment\"\n\nattr(,\"contrasts\")$x2\n[1] \"contr.treatment\"\n\nattr(,\"contrasts\")$x3\n[1] \"contr.treatment\"\n\nattr(,\"contrasts\")$x4\n[1] \"contr.treatment\"\n\n\n\nCompute the correlation in the X matrix. Why are there some missing values in the matrix?\n\n\ncor(x)\n\nWarning in cor(x): the standard deviation is zero\n\n\n            (Intercept) x1+ x2+ x3+ x4+\n(Intercept)           1  NA  NA  NA  NA\nx1+                  NA   1   0   0   0\nx2+                  NA   0   1   0   0\nx3+                  NA   0   0   1   0\nx4+                  NA   0   0   0   1\n\n\n因为截距项全部为1，且其余变量都是01编码\n\nWhat difference in resistance is expected when moving from the low to the high level of x1?\n\n\nsumary(model)\n\n            Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept)  236.781     14.769 16.0322 5.645e-09\nx1+           25.762     13.210  1.9502 0.0770849\nx2+          -69.888     13.210 -5.2906 0.0002561\nx3+           43.587     13.210  3.2996 0.0070828\nx4+          -14.488     13.210 -1.0967 0.2961929\n\nn = 16, p = 5, Residual SE = 26.41970, R-Squared = 0.8\n\n\n\nRefit the model without x4 and examine the regression coefficients and standard errors? What stayed the the same as the original fit and what changed?\n\n\nmodel2 &lt;- lm(resist ~ x1+x2+x3, wafer)\nsumary(model2)\n\n            Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept)  229.538     13.321 17.2312 7.883e-10\nx1+           25.762     13.321  1.9340 0.0770472\nx2+          -69.888     13.321 -5.2464 0.0002056\nx3+           43.587     13.321  3.2721 0.0066773\n\nn = 16, p = 4, Residual SE = 26.64201, R-Squared = 0.78\n\n# x &lt;-  model.matrix(model2)\n# cor(x)\n\n\nExplain how the change in the regression coefficients is related to the correlation matrix of X.\n\n\nx &lt;- model.matrix(model2) # 就是截距项的原因\nsolve(t(x) %*% x) #奇异矩阵，也就是矩阵不可逆 行或列是线性相关的，也就是矩阵的行列式为 0\n\n            (Intercept)    x1+    x2+    x3+\n(Intercept)       0.250 -0.125 -0.125 -0.125\nx1+              -0.125  0.250  0.000  0.000\nx2+              -0.125  0.000  0.250  0.000\nx3+              -0.125  0.000  0.000  0.250\n\n# x &lt;- model.matrix(model2)[,-1] # 就是截距项的原因\n# solve(t(x) %*% x)",
    "crumbs": [
      "Linear regression",
      "linear regression hw 3"
    ]
  },
  {
    "objectID": "linear regression hw 3.html#exercise-8",
    "href": "linear regression hw 3.html#exercise-8",
    "title": "linear regression hw 3",
    "section": "Exercise 8",
    "text": "Exercise 8\nAn experiment was conducted to examine factors that might affect the height of leaf springs in the suspension of trucks. The data may be found in truck. The five factors in the experiment are set to − and + but it will be more convenient for us to use −1 and +1. This can be achieved for the first factor by:\ntruck$B &lt;- sapply(truck$B, function(x) ifelse(x==\"-\",-1,1))\nRepeat for the other four factors.\n\nFit a linear model for the height in terms of the five factors. Report on the value of the regression coefficients.\n\n\ndata(\"truck\")\nglimpse(truck)\n\nRows: 48\nColumns: 6\n$ B      &lt;fct&gt; -, +, -, +, -, +, -, +, -, +, -, +, -, +, -, +, -, +, -, +, -, …\n$ C      &lt;fct&gt; -, -, +, +, -, -, +, +, -, -, +, +, -, -, +, +, -, -, +, +, -, …\n$ D      &lt;fct&gt; -, -, -, -, +, +, +, +, -, -, -, -, +, +, +, +, -, -, -, -, +, …\n$ E      &lt;fct&gt; -, +, +, -, +, -, -, +, -, +, +, -, +, -, -, +, -, +, +, -, +, …\n$ O      &lt;fct&gt; -, -, -, -, -, -, -, -, +, +, +, +, +, +, +, +, -, -, -, -, -, …\n$ height &lt;dbl&gt; 7.78, 8.15, 7.50, 7.59, 7.94, 7.69, 7.56, 7.56, 7.50, 7.88, 7.5…\n\n\n\ntruck &lt;- truck %&gt;% mutate_if(is.factor, ~ ifelse(. == \"-\", -1, 1))\nmodel &lt;- lm(height ~ ., truck)\nsumary(model)\n\n             Estimate Std. Error  t value  Pr(&gt;|t|)\n(Intercept)  7.636042   0.022909 333.3160 &lt; 2.2e-16\nB            0.110625   0.022909   4.8288 1.852e-05\nC           -0.088125   0.022909  -3.8467 0.0004005\nD           -0.014375   0.022909  -0.6275 0.5337453\nE            0.051875   0.022909   2.2644 0.0287751\nO           -0.129792   0.022909  -5.6655 1.201e-06\n\nn = 48, p = 6, Residual SE = 0.15872, R-Squared = 0.64\n\n\n\nFit a linear model using just factors B, C, D and E and report the coefficients. How do these compare to the previous question? Show how we could have anticipated this result by examining the X matrix.\n\n\nmodel2 &lt;- update(model, . ~ . -O)\nsumary(model2)\n\n             Estimate Std. Error  t value  Pr(&gt;|t|)\n(Intercept)  7.636042   0.030073 253.9154 &lt; 2.2e-16\nB            0.110625   0.030073   3.6785 0.0006483\nC           -0.088125   0.030073  -2.9304 0.0054015\nD           -0.014375   0.030073  -0.4780 0.6350709\nE            0.051875   0.030073   1.7250 0.0917167\n\nn = 48, p = 5, Residual SE = 0.20835, R-Squared = 0.37\n\ncor(model.matrix(model2 )) \n\nWarning in cor(model.matrix(model2)): the standard deviation is zero\n\n\n            (Intercept)  B  C  D  E\n(Intercept)           1 NA NA NA NA\nB                    NA  1  0  0  0\nC                    NA  0  1  0  0\nD                    NA  0  0  1  0\nE                    NA  0  0  0  1\n\n\n自变量间无相关性，系数没变\n\nConstruct a new predictor called A which is set to B+C+D+E. Fit a linear model with the predictors A, B, C, D, E and O. Do coefficients for all six predictors appear in the regression summary? Explain.\n\n\ntruck$A &lt;- rowSums(truck[, c(\"B\", \"C\", \"D\", \"O\")])\n\nmodel3 &lt;- update(model, . ~ . + A)\nsumary(model3)\n\n\nCoefficients: (1 not defined because of singularities)\n             Estimate Std. Error  t value  Pr(&gt;|t|)\n(Intercept)  7.636042   0.022909 333.3160 &lt; 2.2e-16\nB            0.110625   0.022909   4.8288 1.852e-05\nC           -0.088125   0.022909  -3.8467 0.0004005\nD           -0.014375   0.022909  -0.6275 0.5337453\nE            0.051875   0.022909   2.2644 0.0287751\nO           -0.129792   0.022909  -5.6655 1.201e-06\n\nn = 48, p = 6, Residual SE = 0.15872, R-Squared = 0.64\n\n\n\nExtract the model matrix X from the previous model. Attempt to compute \\(\\bar \\beta\\) from (XT X)−1XT y. What went wrong and why?\n\nx &lt;- model.matrix(model3)\nsolve(t(x) %*% x)\n\nx &lt;- model.matrix(model3)[,-7] #还是因为奇异性 因为A 不对\nsolve(t(x) %*% x)\n\n            (Intercept)          B          C          D          E          O\n(Intercept)  0.02083333 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\nB            0.00000000 0.02083333 0.00000000 0.00000000 0.00000000 0.00000000\nC            0.00000000 0.00000000 0.02083333 0.00000000 0.00000000 0.00000000\nD            0.00000000 0.00000000 0.00000000 0.02083333 0.00000000 0.00000000\nE            0.00000000 0.00000000 0.00000000 0.00000000 0.02083333 0.00000000\nO            0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.02083333\n\n\n\nUse the QR decomposition method as seen in Section 2.7 to compute \\(\\bar \\beta\\). Are the results satisfactory?\n\n\nqr_decomp &lt;- qr(model.matrix(model3))\nbacksolve(\n  qr.R(qr_decomp), # upper-right\n  t(qr.Q(qr_decomp)) %*% truck$height\n)\n\n              [,1]\n[1,]  7.636042e+00\n[2,]  1.870143e+13\n[3,]  1.870143e+13\n[4,]  1.870143e+13\n[5,]  5.715314e-02\n[6,]  1.870143e+13\n[7,] -1.870143e+13\n\n\n\nUse the function qr.coef to correctly compute \\(\\bar \\beta\\) .\n\n\n#qr.coef 来计算回归系数  是一种有效且稳定的方法，特别是在处理可能存在多重共线性的问题时\nqr.coef(qr_decomp, truck$height)\n\n(Intercept)           B           C           D           E           O \n  7.6360417   0.1106250  -0.0881250  -0.0143750   0.0518750  -0.1297917 \n          A \n         NA",
    "crumbs": [
      "Linear regression",
      "linear regression hw 3"
    ]
  },
  {
    "objectID": "linear regression hw 3.html#exercise-9",
    "href": "linear regression hw 3.html#exercise-9",
    "title": "linear regression hw 3",
    "section": "Exercise 9",
    "text": "Exercise 9",
    "crumbs": [
      "Linear regression",
      "linear regression hw 3"
    ]
  },
  {
    "objectID": "bayesian hw 2.html",
    "href": "bayesian hw 2.html",
    "title": "bayesian hw 2",
    "section": "",
    "text": "EXERCISE 4.15.\nFind Jeffreys’ prior for \\(\\theta\\) based on a random sample of size \\(n\\) when (a) \\(y_i|\\theta ∼ Pois(\\theta)\\), (b) \\(y_i|\\theta ∼ Exp(\\theta)\\), (c) \\(y_i|\\theta ∼ Weib(2,\\theta)\\), (d) \\(y_i|\\theta\\)is negative binomial as in Example 4.3.3.\nAnswer:\n\n\n\n\n\n\nDefinte\n\n\n\nIn general, for a one parameter problem, Fisher’s information is defined to be the expected value of the negative of the second derivative of the log-likelihood. Jeffreys’ prior is defined as being proportional to the square root of the Fisher information.\n\n\n(a) \\(y_i|\\theta ∼ Pois(\\theta)\\)\nWe have, \\[ f(y|\\theta) =  \\theta^y e^{-\\theta y}/y! \\]\nthen,\n\\[\\begin{align}\nL(y |\\theta) &= ln \\frac{\\theta^{\\sum y} e^{-\\theta y}}{\\prod y!}  \\\\\nlog[L(y |\\theta)] &= \\sum y * ln\\theta - \\theta *\\sum y - ln(\\prod y_i !) \\\\\nlog[L(y |\\theta)]' &= \\frac{\\sum y}{\\theta} + \\sum y \\\\\nlog[L(y |\\theta)]'' &= - \\frac{\\sum y}{\\theta^2} \\\\\n-E[log[L(y |\\theta)]''] &= -E[- \\frac{\\sum y}{\\theta^2} ] \\\\\nJ(\\theta) &= \\frac{n \\theta }{\\theta^2} \\\\\nf(\\theta) &\\propto \\theta^{-0.5}\n\n\\end{align}\\]\n(b) \\(y_i|\\theta ∼ Exp(\\theta)\\)\nwe have,\n\\[\nf(y|\\theta) = \\theta e^{−θy}I(0,\\infty)(y)\n\\]\nthen,\n\\[\\begin{align}\nL(y |\\theta) &= \\prod \\theta e^{-\\theta y}  \\\\\nlog[L(y |\\theta)] &= nln\\theta - \\theta \\sum y \\\\\nlog[L(y |\\theta)]' &= \\frac{n}{\\theta} - \\sum y \\\\\nlog[L(y |\\theta)]'' &= - \\frac{n}{\\theta^2} \\\\\n-E[log[L(y |\\theta)]''] &= -E[- \\frac{n}{\\theta^2}] \\\\\nJ(\\theta) &= \\frac{n}{\\theta^2} \\\\\nf(\\theta) &\\propto \\theta^{-1}\n\n\\end{align}\\]\n(c) \\(y_i|\\theta ∼ Weib(2,\\theta)\\)\nwe have,\n\\[\nf(y|\\alpha,\\theta) = 2 \\theta  y * exp (−\\theta y^{2}) I(0,∞)(y)\n\\]\nthen,\n\\[\\begin{align}\n\nL(y |\\theta) &= 2\\theta ^n \\prod y exp(-n \\theta \\sum y^2) \\\\\nlog[L(y |\\theta)] &= nln2\\theta + ln\\prod y -n\\theta \\sum y^2 \\\\\nlog[L(y |\\theta)]' &= \\frac{2n}{2\\theta} -n  \\sum y^2 \\\\\nlog[L(y |\\theta)]'' &= - \\frac{n}{\\theta^2} \\\\\n-E[log[Ln(y |\\theta)]''] &= -E[- \\frac{n}{\\theta^2}] \\\\\nJ(\\theta) &= \\frac{n}{\\theta^2} \\\\\nf(\\theta) &\\propto \\theta^{-1}\n\n\\end{align}\\]\n(d) \\(y_i|\\theta\\) is negative binomial\nwe have,\n\\[\nL(\\theta｜y_2) = \\binom{y_2-1}{s-1}\\theta^{s}(1-\\theta)^{y_2-s}\n\\]\nthen,\n\\[\\begin{align}\nLog(L(\\theta｜y_2)) &\\propto sln(\\theta) + (y_2 -s)ln(1-\\theta)   \\\\\nLog(L(\\theta｜y_2))' &\\propto \\frac{s}{\\theta}-\\frac{y_2-s}{1-\\theta} \\\\\nLog(L(\\theta｜y_2))'' &\\propto  - \\frac{s}{\\theta^2}-\\frac{y_2-s}{(1-\\theta)^2} \\\\\n-E[Log(L(\\theta｜y_2))''] &\\propto -E[- \\frac{s}{\\theta^2}-\\frac{y_2-s}{(1-\\theta)^2}] \\\\\nJ(\\theta) &\\propto \\frac{y_2}{\\theta}  + \\frac{y_2}{1-\\theta}\\\\\np(\\theta)&\\propto \\sqrt{\\frac{1}{\\theta(1-\\theta)}}\n\\end{align}\\]\n\n\nEXERCISE 5.2.\n\nsuppressMessages(library(rjags))\nlibrary(coda)\n\nmodel_string &lt;- \"\n  model {\n    y ~ dbin(theta, n)\n    theta ~ dbeta(12.05, 116.06)\n  }\n\"\n\ndata_list &lt;- list(n = 2430, y = 219)\n\njags_model &lt;- jags.model(textConnection(model_string), \n                         data = data_list, \n                         n.chains = 1,    \n                         n.adapt = 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 1\n   Unobserved stochastic nodes: 1\n   Total graph size: 5\n\nInitializing model\n\nupdate(jags_model, n.iter = 5000)\n\nsamples &lt;- coda.samples(jags_model, \n                        variable.names = c(\"theta\"), \n                        n.iter = 5000)  \n\nsummary(samples)\n\n\nIterations = 10001:15000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n     9.021e-02      5.644e-03      7.982e-05      1.043e-04 \n\n2. Quantiles for each variable:\n\n   2.5%     25%     50%     75%   97.5% \n0.07964 0.08633 0.09017 0.09398 0.10136 \n\n# plot(samples)\n\n\n# Plot the Beta(12.05, 116.06) distribution\nplot(seq(0, 1, length.out = 1000), dbeta(seq(0, 1, length.out = 1000), 12.05, 116.06), \n     type = \"l\", col = \"#4169E1\", lwd = 2, \n     main = \"Beta(12.05, 116.06) vs Beta(231.05, 2327.06)\", \n     ylim = c(0,70),\n     xlab = expression(theta), ylab = \"Density\")\n\n# Add the Beta(231.05, 2327.06) distribution\nlines(seq(0, 1, length.out = 1000), dbeta(seq(0, 1, length.out = 1000),231.05, 2327.06), \n      col = \"red\", lwd = 2, lty = 2)  # Use dashed line type for differentiation\n\n# Add a legend\nlegend(\"topright\", legend = c(\"Beta(12.05, 116.06)\", \"Beta(231.05, 2327.06)\"), \n       col = c(\"#4169E1\", \"red\"), lwd = 2, lty = c(1, 2))\n\n\n\n\n\n\n\n\n\n\nEXERCISE 5.3.\nUsing calculus, find the mode and 5th percentile of a Beta(10,1) distribution.\n\\[\\begin{align}\nbeta(10,1) &= 10\\theta^9 \\\\\nf(\\theta)'&=90\\theta^8\n\\end{align}\\] 单调递增 mode = 1\n\\[\\begin{align}\nq&=\\int_{0}^{c} f(\\theta)d\\theta \\\\\nq&= c^{10}\n\\end{align}\\]\n5 分位数为 \\(0.05^{10}\\)\n\n\nEXERCISE 5.4.\nUsing calculus, find a and b such that a Beta(a, b) distribution has a mode of 1 and a 5th percentile of 0.2. mode=\\((a-1)/(a+b-2)\\),令其=1，则b=1\n\\[\\begin{align}\n0.2 &=\\int_{0}^{0.05} a \\theta^{1-1} d\\theta \\\\\nq&= c^{10} \\\\\n0.05^a&=0.2 \\\\\na&=ln(0.2)/ln(0.05)\n\\end{align}\\]\n\n\nEXERCISE 5.5.\nDerive formula (1), including the formula for θ0.\n\\[\\begin{align}\n\\theta_0 &= \\frac{a-1}{a+b-2} \\\\\na(\\theta_0 -1) &= 2\\theta_0 - b\\theta_0 -1 \\\\\na &= \\frac{2\\theta_0 - b\\theta_0 - 1}{\\theta_0 -1}  \n\\end{align}\\]\n\n\nEXERCISE 5.6.\nUse BetaBuster to find the Beta(a, b) priors for mode 0.75 and 5th percentile 0.60, and for mode 0.01 and 99th percentile 0.02. What is the Beta prior when the mode is 1 and the first percentile is 0.80?\n\n\n\n\n\nEXERCISE 5.7.\nThe distributions θ ∼ Beta(1.6, 1) and θ ∼ Beta(1, 0.577) both have a mode of 1. Find Pr[θ &lt; 0.5] analytically for each. Does BetaBuster give the appropriate parameters for the Beta distributions?\n\npbeta(0.5,1.6,1)\n\n[1] 0.329877\n\npbeta(0.5,1,0.577)\n\n[1] 0.3296437\n\n\n\n\nEXERCISE 5.8.\n\nmodel_string &lt;- \"\n  model{ \n  gamma[1] ~ dbeta(a1,b1) \n  gamma[2] ~ dbeta(a2,b2) \n  theta &lt;- w*gamma[1] + (1-w)*gamma[2]\n  w ~ dbern(p) }\n\"\n\ndata_list &lt;- list(a1 = 10, b1 = 20, a2 = 20, b2 = 10, p=0.5 )\n\njags_model &lt;- jags.model(textConnection(model_string), \n                         data = data_list, \n                         n.chains = 1,    \n                         n.adapt = 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 0\n   Unobserved stochastic nodes: 3\n   Total graph size: 13\n\nInitializing model\n\nupdate(jags_model, n.iter = 5000)\n\nsamples &lt;- coda.samples(jags_model, \n                        variable.names = c(\"theta\"), \n                        n.iter = 5000)  \n\n# summary(samples)\n\nhist(samples[[1]])\n\n\n\n\n\n\n\n\n\nmodel_string &lt;- \"\n  model{ \n  gamma[1] ~ dbeta(a1,b1) \n  gamma[2] ~ dbeta(a2,b2) \n  theta &lt;- w*gamma[1] + (1-w)*gamma[2]\n  w ~ dbern(p) }\n\"\n\ndata_list &lt;- list(a1 = 10, b1 = 20, a2 = 20, b2 = 10, p=0.2 )\n\njags_model &lt;- jags.model(textConnection(model_string), \n                         data = data_list, \n                         n.chains = 1,    \n                         n.adapt = 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 0\n   Unobserved stochastic nodes: 3\n   Total graph size: 13\n\nInitializing model\n\nupdate(jags_model, n.iter = 5000)\n\nsamples &lt;- coda.samples(jags_model, \n                        variable.names = c(\"theta\"), \n                        n.iter = 5000)  \n\n# summary(samples)\n\nhist(samples[[1]])\n\n\n\n\n\n\n\n\n\nmodel_string &lt;- \"\n  model{ \n  gamma[1] ~ dbeta(a1,b1) \n  gamma[2] ~ dbeta(a2,b2) \n  theta &lt;- w*gamma[1] + (1-w)*gamma[2]\n  w ~ dbern(p) }\n\"\n\ndata_list &lt;- list(a1 = 1, b1 = 1, a2 = 20, b2 = 10, p=0.5 )\n\njags_model &lt;- jags.model(textConnection(model_string), \n                         data = data_list, \n                         n.chains = 1,    \n                         n.adapt = 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 0\n   Unobserved stochastic nodes: 3\n   Total graph size: 13\n\nInitializing model\n\nupdate(jags_model, n.iter = 5000)\n\nsamples &lt;- coda.samples(jags_model, \n                        variable.names = c(\"theta\"), \n                        n.iter = 5000)  \n\n# summary(samples)\n\nhist(samples[[1]])\n\n\n\n\n\n\n\n\n\nmodel_string &lt;- \"\n  model{ \n  gamma[1] ~ dbeta(a1,b1) \n  gamma[2] ~ dbeta(a2,b2) \n  theta &lt;- w*gamma[1] + (1-w)*gamma[2]\n  w ~ dbern(p) }\n\"\n\ndata_list &lt;- list(a1 = 0.1, b1 =0.2, a2 = 20, b2 = 10, p=0.5 )\n\njags_model &lt;- jags.model(textConnection(model_string), \n                         data = data_list, \n                         n.chains = 1,    \n                         n.adapt = 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 0\n   Unobserved stochastic nodes: 3\n   Total graph size: 13\n\nInitializing model\n\nupdate(jags_model, n.iter = 5000)\n\nsamples &lt;- coda.samples(jags_model, \n                        variable.names = c(\"theta\"), \n                        n.iter = 5000)  \n\n# summary(samples)\n\nhist(samples[[1]])\n\n\n\n\n\n\n\n\n\n\nEXERCISE 5.9.\n各种调整参数 测试\n\nmodel_string &lt;- \"\nmodel{\ntheta ~ dbeta(a,b)I(,t)\n}\n\"\ndata_list &lt;- list(a=2,b=3,t=0.9)\ninits &lt;- list(theta = 0.5)\njags_model &lt;- jags.model(textConnection(model_string),\n                      data=data_list,\n                      n.chains=1,\n                      n.adapt = 1000,\n                      inits = inits)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 0\n   Unobserved stochastic nodes: 1\n   Total graph size: 4\n\nInitializing model\n\nupdate(jags_model,n.iter = 1000)\nsamples &lt;- coda.samples(jags_model,\n                        variable.names = c('theta'),\n                        n.iter = 5000)\nhist(samples[[1]])\n\n\n\n\n\n\n\n\n\n\nEXERCISE 5.12.\n\nmodel_string &lt;- \"\nmodel{\ny[1] ~ dbin(theta[1],n[1]) \ny[2] ~ dbin(theta[2],n[2]) \n\n# betabuster\ntheta[1] ~ dbeta(3.2846,6.3307) \n\ntheta[2] ~ dbeta(1.5317,1.5317) \n\n\nodds[1] &lt;- theta[1]/(1-theta[1]) \nodds[2] &lt;- theta[2]/(1-theta[2]) \nRD &lt;- theta[2]-theta[1] \nRR &lt;- theta[2]/theta[1] \nOR &lt;- odds[2]/odds[1] \ntest &lt;- step(RD)\n}\n\"\n\ndata_list &lt;- list(n=c(134,400), y=c(54,224)) \n\njags_model &lt;- jags.model(textConnection(model_string),\n                      data=data_list,\n                      n.chains=1,\n                      n.adapt = 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 2\n   Unobserved stochastic nodes: 2\n   Total graph size: 18\n\nInitializing model\n\nupdate(jags_model,n.iter = 5000)\n\nsamples &lt;- coda.samples(jags_model,\n                        variable.names = c('theta'),\n                        n.iter = 5000)\nsummary(samples)\n\n\nIterations = 10001:15000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean      SD  Naive SE Time-series SE\ntheta[1] 0.3992 0.04088 0.0005781      0.0007396\ntheta[2] 0.5596 0.02470 0.0003493      0.0004355\n\n2. Quantiles for each variable:\n\n           2.5%    25%    50%    75%  97.5%\ntheta[1] 0.3189 0.3719 0.3987 0.4263 0.4800\ntheta[2] 0.5107 0.5432 0.5598 0.5759 0.6076\n\n\n对比发现区别不大，收数据影响大\n\n\nEXERCISE 5.14.\n\nmodel_string &lt;- \"\nmodel{\ny[1] ~ dbin(theta[1],n[1]) \ny[2] ~ dbin(theta[2],n[2]) \n\ntheta[1] ~ dunif(0,1) \ntheta[2] ~ dunif(0,1) \n\nodds[1] &lt;- theta[1]/(1-theta[1]) \nodds[2] &lt;- theta[2]/(1-theta[2]) \n\nOR &lt;- odds[2]/odds[1] \n}\n\"\n\ndata_list &lt;- list(n=c(7,16), y=c(7,8)) \n\njags_model &lt;- jags.model(textConnection(model_string),\n                      data=data_list,\n                      n.chains=1,\n                      n.adapt = 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 2\n   Unobserved stochastic nodes: 2\n   Total graph size: 13\n\nInitializing model\n\nupdate(jags_model,n.iter = 5000)\n\nsamples &lt;- coda.samples(jags_model,\n                        variable.names = c('OR'),\n                        n.iter = 5000)\nsummary(samples)\n\n\nIterations = 10001:15000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n      0.147890       0.205371       0.002904       0.004971 \n\n2. Quantiles for each variable:\n\n    2.5%      25%      50%      75%    97.5% \n0.002124 0.029880 0.080170 0.186287 0.683988 \n\n\n关联并不能代表因果\n\n\nEXERCISE 5.15.\n\nmodel_string &lt;- \"\nmodel{ \nfor(i in 1:2){ y[i] ~ dbin(theta[i],n[i]) } \ntheta[2] ~ dbeta(a,b) \ndelta ~ dnorm(mu, prec) \ntheta[1] &lt;- exp(delta)*theta[2]/(1-theta[2]*(1-exp(delta))) \nOR &lt;- theta[1]/(1-theta[1])/(theta[2]/(1-theta[2])) }\n\"\n\ndata_list &lt;- list(n=c(7,16), y=c(7,8),a=0.5,b=0.5,mu=2, prec=1/2)\n\njags_model &lt;- jags.model(textConnection(model_string),\n                      data=data_list,\n                      n.chains=1,\n                      n.adapt = 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 2\n   Unobserved stochastic nodes: 2\n   Total graph size: 22\n\nInitializing model\n\nupdate(jags_model,n.iter = 5000)\n\nsamples &lt;- coda.samples(jags_model,\n                        variable.names = c('OR'),\n                        n.iter = 5000)\nsummary(samples)\n\n\nIterations = 10001:15000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n       38.4742        69.0256         0.9762         1.4440 \n\n2. Quantiles for each variable:\n\n   2.5%     25%     50%     75%   97.5% \n  2.392   8.873  18.488  39.392 207.883 \n\nmean(samples[[1]]&gt;1)\n\n[1] 0.9974\n\nmean(samples[[1]]&gt;2)\n\n[1] 0.9842\n\n\n\n\nEXERCISE 5.16.\n\nmodel_string &lt;- \"\nmodel {\n  for (i in 1:2) {\n    y[i] ~ dbin(theta[i], n[i]) \n  }\n\n  theta_t1 ~ dbeta(13.3221, 6.2809)  \n  theta_t2 ~ dbeta(6.2809,13.3221)              \n  gamma ~ dbeta(2, 2)                \n\n  theta[1] &lt;- theta_t1 * gamma / (theta_t1 * gamma + theta_t2 * (1 - gamma))  \n  theta[2] &lt;- (1 - theta_t1) * gamma / ((1 - theta_t1) * gamma + (1 - theta_t2) * (1 - gamma))  \n  \n  OR &lt;- theta[1]/(1-theta[1])/(theta[2]/(1-theta[2])) }\n\n\"\n\ndata_list &lt;- list(n=c(7,16), y=c(7,8))\n\njags_model &lt;- jags.model(textConnection(model_string),\n                      data=data_list,\n                      n.chains=1,\n                      n.adapt = 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 2\n   Unobserved stochastic nodes: 3\n   Total graph size: 27\n\nInitializing model\n\nupdate(jags_model,n.iter = 5000)\n\nsamples &lt;- coda.samples(jags_model,\n                        variable.names = c('OR'),\n                        n.iter = 5000)\nsummary(samples)\n\n\nIterations = 10001:15000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n       8.10275        6.02819        0.08525        0.10830 \n\n2. Quantiles for each variable:\n\n  2.5%    25%    50%    75%  97.5% \n 2.041  4.297  6.535  9.969 23.207 \n\nmean(samples[[1]]&gt;1)\n\n[1] 1\n\nmean(samples[[1]]&gt;2)\n\n[1] 0.9766\n\n\n\nmodel_string &lt;- \"\nmodel {\n  for (i in 1:2) {\n    y[i] ~ dbin(theta[i], n[i]) \n  }\n\n  theta_t1 ~ dbeta(28.3393, 42.009)  \n  theta_t2 ~ dbeta(42.009,28.3393)              \n  gamma ~ dbeta(2, 2)                \n\n  theta[1] &lt;- theta_t1 * gamma / (theta_t1 * gamma + theta_t2 * (1 - gamma))  \n  theta[2] &lt;- (1 - theta_t1) * gamma / ((1 - theta_t1) * gamma + (1 - theta_t2) * (1 - gamma))  \n  \n  OR &lt;- theta[1]/(1-theta[1])/(theta[2]/(1-theta[2])) }\n\n\"\n\ndata_list &lt;- list(n=c(7,16), y=c(7,8))\n\njags_model &lt;- jags.model(textConnection(model_string),\n                      data=data_list,\n                      n.chains=1,\n                      n.adapt = 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 2\n   Unobserved stochastic nodes: 3\n   Total graph size: 27\n\nInitializing model\n\nupdate(jags_model,n.iter = 5000)\n\nsamples &lt;- coda.samples(jags_model,\n                        variable.names = c('OR'),\n                        n.iter = 5000)\nsummary(samples)\n\n\nIterations = 10001:15000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n      0.669130       0.220485       0.003118       0.003779 \n\n2. Quantiles for each variable:\n\n  2.5%    25%    50%    75%  97.5% \n0.3427 0.5120 0.6352 0.7941 1.1902 \n\nmean(samples[[1]]&gt;1)\n\n[1] 0.0718\n\nmean(samples[[1]]&gt;2)\n\n[1] 0\n\n\n\n\nEXERCISE 5.17.\n直接在代码了里面添加对应的需要的参数即可\n\n\nEXERCISE 5.18.\n\n# 重复5.14 计算出 theta1 2 的后验分布\nmodel_string &lt;- \"\nmodel{\ny[1] ~ dbin(theta[1],n[1]) \ny[2] ~ dbin(theta[2],n[2]) \n\ntheta[1] ~ dunif(0,1) \ntheta[2] ~ dunif(0,1) \n\n}\n\"\n\ndata_list &lt;- list(n=c(7,16), y=c(7,8)) \n\njags_model &lt;- jags.model(textConnection(model_string),\n                      data=data_list,\n                      n.chains=1,\n                      n.adapt = 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 2\n   Unobserved stochastic nodes: 2\n   Total graph size: 8\n\nInitializing model\n\nupdate(jags_model,n.iter = 5000)\n\nsamples &lt;- coda.samples(jags_model,\n                        variable.names = c('theta[1]','theta[2]'),\n                        n.iter = 5000)\nsummary(samples)\n\n\nIterations = 10001:15000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean      SD Naive SE Time-series SE\ntheta[1] 0.8885 0.09962 0.001409       0.002807\ntheta[2] 0.4992 0.11522 0.001629       0.001980\n\n2. Quantiles for each variable:\n\n           2.5%    25%    50%    75%  97.5%\ntheta[1] 0.6291 0.8398 0.9164 0.9646 0.9964\ntheta[2] 0.2745 0.4210 0.5003 0.5784 0.7202\n\n\n\n# 更新\nmodel_string &lt;- \"\nmodel{\ny[1] ~ dbin(theta[1],n[1]) \ny[2] ~ dbin(theta[2],n[2]) \n\ntheta[1] ~ dunif(0,0.9169) \ntheta[2] ~ dunif(0,0.5027) \n\nodds[1] &lt;- theta[1]/(1-theta[1]) \nodds[2] &lt;- theta[2]/(1-theta[2]) \n\nOR &lt;- odds[2]/odds[1] \n}\n\"\n\ndata_list &lt;- list(n=c(37,75), y=c(35,32)) \n\njags_model &lt;- jags.model(textConnection(model_string),\n                      data=data_list,\n                      n.chains=1,\n                      n.adapt = 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 2\n   Unobserved stochastic nodes: 2\n   Total graph size: 15\n\nInitializing model\n\nupdate(jags_model,n.iter = 5000)\n\nsamples &lt;- coda.samples(jags_model,\n                        variable.names = c('OR'),\n                        n.iter = 5000)\nsummary(samples)\n\n\nIterations = 10001:15000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n     0.1017486      0.0398543      0.0005636      0.0010154 \n\n2. Quantiles for each variable:\n\n   2.5%     25%     50%     75%   97.5% \n0.05120 0.07443 0.09265 0.11903 0.20158 \n\n\n敏感性分析 换不同的先验 结果差不多\n\n\nEXERCISE 5.23.\n\nmodel_string &lt;- \"\nmodel{ \nfor(i in 1:n){ y[i] ~ dnorm(mu, tau) } \n# mu ~ dnorm(0, 1/1000000) \n mu ~ dnorm(4.75, 1/0.0163) \ntau ~ dgamma(c,d) \nsigma &lt;- 1/sqrt(tau)\ngamma &lt;- phi((4.4-mu)/sqrt(1/tau)) \nprob &lt;- step(4.4 - y[13]) }  \n\"\n\ndata_list &lt;- list(y=c(4.20,4.36,4.11,3.96,5.63,4.50, 5.64,4.38,4.45,3.67,5.26,4.66,NA),\n                  c=0.001, d=0.001,n=13) \n\n\njags_model &lt;- jags.model(textConnection(model_string),\n                      data=data_list,\n                      n.chains=1,\n                      n.adapt = 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 12\n   Unobserved stochastic nodes: 3\n   Total graph size: 32\n\nInitializing model\n\nupdate(jags_model,n.iter = 5000)\n\nsamples &lt;- coda.samples(jags_model,\n                        variable.names = c('mu','sigma','y[13]','gamma','prob'),\n                        n.iter = 5000)\nsummary(samples[[1]])\n\n\nIterations = 5001:10000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n        Mean      SD  Naive SE Time-series SE\ngamma 0.3279 0.06218 0.0008794      0.0008532\nmu    4.6926 0.10581 0.0014963      0.0015487\nprob  0.3342 0.47176 0.0066717      0.0066717\nsigma 0.6689 0.15071 0.0021314      0.0021951\ny[13] 4.7024 0.68050 0.0096237      0.0096237\n\n2. Quantiles for each variable:\n\n        2.5%    25%    50%    75%  97.5%\ngamma 0.2079 0.2864 0.3287 0.3703 0.4476\nmu    4.4919 4.6211 4.6904 4.7631 4.9070\nprob  0.0000 0.0000 0.0000 1.0000 1.0000\nsigma 0.4438 0.5628 0.6451 0.7521 1.0240\ny[13] 3.4037 4.2408 4.7074 5.1384 6.0625\n\nplot(samples[[1]][,'gamma'])\n\n\n\n\n\n\n\n\n\nmodel_string &lt;- \"\nmodel{ \nfor(i in 1:n){ y[i] ~ dnorm(mu, tau) } \n mu ~ dnorm(0, 1/1000000) \n# mu ~ dnorm(4.75, 1/0.0163) \ntau ~ dgamma(c,d) \nsigma &lt;- 1/sqrt(tau)\ngamma &lt;- phi((4.4-mu)/sqrt(1/tau)) \nprob &lt;- step(4.4 - y[13]) }  \n\"\n\ndata_list &lt;- list(y=c(4.20,4.36,4.11,3.96,5.63,4.50, 5.64,4.38,4.45,3.67,5.26,4.66,NA),\n                  c=0.001, d=0.001,n=13) \n\n\njags_model &lt;- jags.model(textConnection(model_string),\n                      data=data_list,\n                      n.chains=1,\n                      n.adapt = 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 12\n   Unobserved stochastic nodes: 3\n   Total graph size: 32\n\nInitializing model\n\nupdate(jags_model,n.iter = 5000)\n\nsamples &lt;- coda.samples(jags_model,\n                        variable.names = c('mu','sigma','y[13]','gamma','prob'),\n                        n.iter = 5000)\nsummary(samples[[1]])\n\n\nIterations = 5001:10000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n        Mean     SD Naive SE Time-series SE\ngamma 0.4022 0.1089 0.001540       0.001540\nmu    4.5674 0.2034 0.002876       0.002876\nprob  0.3912 0.4881 0.006902       0.006902\nsigma 0.6810 0.1665 0.002355       0.002617\ny[13] 4.5814 0.7251 0.010255       0.010255\n\n2. Quantiles for each variable:\n\n        2.5%    25%    50%    75%  97.5%\ngamma 0.2032 0.3233 0.3973 0.4778 0.6247\nmu    4.1764 4.4355 4.5672 4.6932 4.9676\nprob  0.0000 0.0000 0.0000 1.0000 1.0000\nsigma 0.4453 0.5664 0.6528 0.7657 1.0713\ny[13] 3.1581 4.1241 4.5776 5.0337 6.0350\n\nplot(samples[[1]][,'gamma'])\n\n\n\n\n\n\n\n\n\n\nEXERCISE 5.24.\nSolve for b above when the lower 5th percentile is specified by the expert. \\[\na − 1.645 \\sqrt \\frac{1}{b} = l\n\\]\n90% upper\n\\[\na + 1.281552 \\sqrt \\frac{1}{b} = u\n\\]\n90% lower \\[\na - 1.281552 \\sqrt \\frac{1}{b} = l\n\\]\nfind the bs for u = 70 and l = 58\n\na &lt;- 65\nb &lt;- 1/0.0163\nu &lt;- 70\nl &lt;- 58\nb_u = (1.281552/(u - a))^2\n\nb_l = (1.281552/(a - l))^2\n\nb_u\n\n[1] 0.06569502\n\nb_l\n\n[1] 0.03351787\n\n\n\n\nEXERCISE 5.25.\n\n在设置先验的时候，\\(\\mu\\) 单独一个分布 \\(\\tau\\) 单独一个分布 ，两者是独立的\n后验分布更新的时候 两者概率也是独立的\\(p(μ,τ) = p(μ)p(τ).\\)\n\n\n\nEXERCISE 5.26.\n\n\n\n\n#1 R Code for finding prior on sigma \nalpha &lt;- 0.90 \nbeta &lt;- 0.95 \na &lt;- 65 # Best guess for mu \ntildegamma &lt;- 85 # Best guess for gamma_alpha\ntildeu &lt;- 91 # Best guess percentile of gamma_alpha \nzalpha &lt;- 1.28 # qnorm(0.90,0,1) \nf &lt;- 2.588 # Initial value for f \n# Could use a sequence of values, say f &lt;- seq(1,50,1) \nsigma0 &lt;- (tildegamma - a)/zalpha\ne &lt;- 1 + sigma0*f \n# We must find the Gamma(e,f) distribution that\n# has beta-percentile = tildesigmabeta \ntildesigmabeta &lt;- (tildeu - a)/zalpha \ntrialq &lt;- qgamma(beta,e,f) # Return beta-percentile for the \n                           # selected gamma distribution \ntrialq # If trialq = tildesigmabeta \n\n[1] 20.31031\n\ntildesigmabeta # stop and pick corresponding f\n\n[1] 20.3125\n\n\n\n\n\n\n#2 R Code for finding prior on tau \nalpha &lt;- 0.90\nbeta &lt;- 0.95\nzalpha &lt;- 1.28 \na &lt;- 65 \ntildegamma &lt;- 85 \ntildel &lt;- 79  # 最后一问这里改成70\nd &lt;- 1328\ntau0 = (zalpha/(tildegamma - a))^2\nc = 1 + tau0*d \ntildetaubeta = (zalpha/(tildel - a))^2 \ntrialq = qgamma(beta,c,d) \ntrialq \n\n[1] 0.008358841\n\ntildetaubeta\n\n[1] 0.008359184\n\n\n\n\n\n\nplot(seq(0, 60, length.out = 1000), \n     dgamma(seq(0, 100, length.out = 1000), shape = 41.4375, rate = 2.588), \n     type = \"l\", main = \"Density of σ ~ Gamma(41.4375, 2.588)\", col = \"blue\", lwd = 2)\n\ntau_samples &lt;- rgamma(10000, shape = 6.439, rate = 1328)\nsigma_samples &lt;- (1 / tau_samples)^0.5\n\n# 绘制 σ 的密度图\nlines(density(sigma_samples), col = \"red\", lwd = 2)\n\n# 添加图例\nlegend(\"topright\", legend = c(\" σ ~ Gamma(41.4375, 2.588)\", \n                               \"τ ~ Gamma(6.439, 1328)\"),\n       col = c(\"blue\", \"red\"), lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nyour best guess for the mean exam score is 60,\n\n\n#1 R Code for finding prior on sigma \nalpha &lt;- 0.90 \nbeta &lt;- 0.95 \na &lt;- 60 # Best guess for mu \ntildegamma &lt;- 85 # Best guess for gamma_alpha\ntildeu &lt;- 91 # Best guess percentile of gamma_alpha \nzalpha &lt;- 1.28 # qnorm(0.90,0,1) \nf &lt;- 3.068 # Initial value for f   # 二分法手调 懒得写函数了\n# Could use a sequence of values, say f &lt;- seq(1,50,1) \nsigma0 &lt;- (tildegamma - a)/zalpha\ne &lt;- 1 + sigma0*f \n# We must find the Gamma(e,f) distribution that\n# has beta-percentile = tildesigmabeta \ntildesigmabeta &lt;- (tildeu - a)/zalpha \ntrialq &lt;- qgamma(beta,e,f) # Return beta-percentile for the \n                           # selected gamma distribution \ntrialq # If trialq = tildesigmabeta \n\n[1] 24.21879\n\ntildesigmabeta # stop and pick corresponding f\n\n[1] 24.21875\n\ne\n\n[1] 60.92188\n\n\n\na=60\nu=70\nb &lt;- (1.645/(u-a))^2 \nb\n\n[1] 0.02706025\n\n\n\n\n\n\n\n\nNote\n\n\n\n\\(\\mu ～ N(60, 1/0.02706025)\\)\n\\(\\sigma ～ Gamma(60.92188, 3.068)\\)\n\n\n\nyou are 95% sure that the mean exam score is less than 65,\n\n\na=60\nu=65\nb &lt;- (1.645/(u-a))^2 \nb\n\n[1] 0.108241\n\n\n\n#1 R Code for finding prior on sigma \nalpha &lt;- 0.90 \nbeta &lt;- 0.95 \na &lt;- 60 # Best guess for mu \ntildegamma &lt;- 85 # Best guess for gamma_alpha\ntildeu &lt;- 91 # Best guess percentile of gamma_alpha \nzalpha &lt;- 1.28 # qnorm(0.90,0,1) \nf &lt;- 3.068 # Initial value for f   # 二分法手调 懒得写函数了\n# Could use a sequence of values, say f &lt;- seq(1,50,1) \nsigma0 &lt;- (tildegamma - a)/zalpha\ne &lt;- 1 + sigma0*f \n# We must find the Gamma(e,f) distribution that\n# has beta-percentile = tildesigmabeta \ntildesigmabeta &lt;- (tildeu - a)/zalpha \ntrialq &lt;- qgamma(beta,e,f) # Return beta-percentile for the \n                           # selected gamma distribution \ntrialq # If trialq = tildesigmabeta \n\n[1] 24.21879\n\ntildesigmabeta # stop and pick corresponding f\n\n[1] 24.21875\n\ne\n\n[1] 60.92188\n\n\n\n\n\n\n\n\nNote\n\n\n\n\\(\\mu ～ N(60, 1/0.108241)\\)\n\\(\\sigma ～ Gamma(60.92188, 3.068)\\)\n\n\n\nyour best guess for the 90th percentile of exam scores is 80, and\n\n\na=60\nu=65\nb &lt;- (1.645/(u-a))^2 \nb\n\n[1] 0.108241\n\n\n\n#1 R Code for finding prior on sigma \nalpha &lt;- 0.90 \nbeta &lt;- 0.95 \na &lt;- 60 # Best guess for mu \ntildegamma &lt;- 80 # Best guess for gamma_alpha\ntildeu &lt;- 91 # Best guess percentile of gamma_alpha \nzalpha &lt;- 1.28 # qnorm(0.90,0,1) \nf &lt;- 0.932# Initial value for f   # 二分法手调 懒得写函数了\n# Could use a sequence of values, say f &lt;- seq(1,50,1) \nsigma0 &lt;- (tildegamma - a)/zalpha\ne &lt;- 1 + sigma0*f \n# We must find the Gamma(e,f) distribution that\n# has beta-percentile = tildesigmabeta \ntildesigmabeta &lt;- (tildeu - a)/zalpha \ntrialq &lt;- qgamma(beta,e,f) # Return beta-percentile for the \n                           # selected gamma distribution \ntrialq # If trialq = tildesigmabeta \n\n[1] 24.21494\n\ntildesigmabeta # stop and pick corresponding f\n\n[1] 24.21875\n\ne\n\n[1] 15.5625\n\n\n\n\n\n\n\n\nNote\n\n\n\n\\(\\mu ～ N(60, 1/0.108241)\\)\n\\(\\sigma ～ Gamma(15.5625, 0.932)\\)\n\n\n\nyou are 95% sure that the 90th percentile is less than 90\n\n\na=60\nu=65\nb &lt;- (1.645/(u-a))^2 \nb\n\n[1] 0.108241\n\n\n\n#1 R Code for finding prior on sigma \nalpha &lt;- 0.90 \nbeta &lt;- 0.95 \na &lt;- 60 # Best guess for mu \ntildegamma &lt;- 80 # Best guess for gamma_alpha\ntildeu &lt;- 90 # Best guess percentile of gamma_alpha \nzalpha &lt;- 1.28 # qnorm(0.90,0,1) \nf &lt;- 1.089 # Initial value for f   # 二分法手调 懒得写函数了\n# Could use a sequence of values, say f &lt;- seq(1,50,1) \nsigma0 &lt;- (tildegamma - a)/zalpha\ne &lt;- 1 + sigma0*f \n# We must find the Gamma(e,f) distribution that\n# has beta-percentile = tildesigmabeta \ntildesigmabeta &lt;- (tildeu - a)/zalpha \ntrialq &lt;- qgamma(beta,e,f) # Return beta-percentile for the \n                           # selected gamma distribution \ntrialq # If trialq = tildesigmabeta \n\n[1] 23.43242\n\ntildesigmabeta # stop and pick corresponding f\n\n[1] 23.4375\n\ne\n\n[1] 18.01562\n\n\n\n\n\n\n\n\nNote\n\n\n\n\\(\\mu ～ N(60, 1/0.108241)\\)\n\\(\\sigma ～ Gamma(18.01562, 1.089)\\)\n\n\n\n\n\nb中对应部分修改为70\n tildel &lt;- 79  # 最后一问这里改成70\n\n\nEXERCISE 5.29.\n\nrun_jags_model &lt;- function(n1, mu1, tau1, n2, mu2, tau2, a1, b1, c1, d1, a2, b2, c2, d2) {\n\n  y &lt;- rnorm(n1, mu1, sqrt(1/tau1))\n  x &lt;- rnorm(n2, mu2, sqrt(1/tau2))\n  \n  model_string &lt;- \"\n  model {\n    for (i in 1:n[1]) {\n      y[i] ~ dnorm(mu[1], tau[1])\n    }\n    for (j in 1:n[2]) {\n      x[j] ~ dnorm(mu[2], tau[2])\n    }\n    \n    for (r in 1:2) {\n      mu[r] ~ dnorm(a[r], b[r])\n      tau[r] ~ dgamma(c[r], d[r])\n      sigma[r] &lt;- sqrt(1/tau[r])\n    }\n    \n    meandiff &lt;- mu[1] - mu[2]\n    sdratio &lt;- sigma[1] / sigma[2]\n    \n    prob[1] &lt;- step(meandiff) # Pr(meandiff &gt; 0 | data)\n    prob[2] &lt;- step(sdratio - 1) # Pr(sdratio &gt; 1 | data)\n  }\"\n  \n    data_list &lt;- list(y = y, x = x, n = c(n1, n2), \n                    a = c(a1, a2), b = c(b1, b2), \n                    c = c(c1, c2), d = c(d1, d2))\n  \n  \n  # 运行JAGS模型\n  jags_model &lt;- jags.model(textConnection(model_string), data = data_list, \n                            n.chains = 1, n.adapt = 5000)\n  \n  update(jags_model , n.iter = 5000)\n  \n  # 提取结果\n  results &lt;- coda.samples(jags_model, variable.names = c(\"mu\", \"tau\", \"meandiff\", \"sdratio\", \"prob\"), n.iter = 5000)\n  \n  return(results)\n}\n\nset.seed(2024) \nn1 &lt;- 30     # 第一组的样本大小\nmu1 &lt;- 5     # 第一组的均值\ntau1 &lt;- 1    # 第一组的精度\n\nn2 &lt;- 20     # 第二组的样本大小\nmu2 &lt;- 3     # 第二组的均值\ntau2 &lt;- 0.5  # 第二组的精度\n\n# 两个分布的参考先验参数\na1 &lt;- 0; b1 &lt;- 0.001; c1 &lt;- 0.001; d1 &lt;- 0.001\na2 &lt;- 0; b2 &lt;- 0.001; c2 &lt;- 0.001; d2 &lt;- 0.001\n\n# 运行JAGS模型\nresults &lt;- run_jags_model(n1, mu1, tau1, n2, mu2, tau2, a1, b1, c1, d1, a2, b2, c2, d2)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 50\n   Unobserved stochastic nodes: 4\n   Total graph size: 74\n\nInitializing model\n\n# 结果总结\nsummary(results)\n\n\nIterations = 5001:10000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean      SD Naive SE Time-series SE\nmeandiff 1.5972 0.37132 0.005251       0.004945\nmu[1]    4.7872 0.22017 0.003114       0.003114\nmu[2]    3.1900 0.30329 0.004289       0.004289\nprob[1]  0.9998 0.01414 0.000200       0.000200\nprob[2]  0.2760 0.44706 0.006322       0.006511\nsdratio  0.8987 0.19271 0.002725       0.002801\ntau[1]   0.7645 0.20307 0.002872       0.003044\ntau[2]   0.6009 0.19615 0.002774       0.002884\n\n2. Quantiles for each variable:\n\n           2.5%    25%    50%    75% 97.5%\nmeandiff 0.8772 1.3493 1.5947 1.8490 2.319\nmu[1]    4.3581 4.6398 4.7879 4.9276 5.224\nmu[2]    2.6007 2.9884 3.1900 3.3908 3.786\nprob[1]  1.0000 1.0000 1.0000 1.0000 1.000\nprob[2]  0.0000 0.0000 0.0000 1.0000 1.000\nsdratio  0.5712 0.7621 0.8823 1.0184 1.323\ntau[1]   0.4177 0.6169 0.7491 0.8916 1.209\ntau[2]   0.2781 0.4585 0.5799 0.7219 1.039\n\n\n\n\nEXERCISE 5.30.\n\nmodel_string &lt;- \"\nmodel {\n  for(i in 1:n[1]) {\n    low[i] ~ dlnorm(mu[1], tau[1])\n  }\n  for(i in 1:n[2]) {\n    normal[i] ~ dlnorm(mu[2], tau[2])\n  }\n\n  # 使用不同的先验分布\n  mu[1] ~ dnorm(0, 0.00001) # 非信息性先验\n  mu[2] ~ dnorm(0, 0.00001) # 非信息性先验\n  \n  tau[1] ~ dgamma(0.001, 0.001) # 非信息性先验\n  tau[2] ~ dgamma(0.001, 0.001) # 非信息性先验\n  \n  med[1] &lt;- exp(mu[1]) \n  med[2] &lt;- exp(mu[2]) \n  rmed &lt;- med[2] / med[1] \n\n  test[1] &lt;- step(med[2] - med[1]) \n  test[2] &lt;- step(Nf - Lf) \n  \n  Lf ~ dlnorm(mu[1], tau[1]) \n  Nf ~ dlnorm(mu[2], tau[2]) \n  \n  dmu &lt;- mu[2] - mu[1] \n  rtau &lt;- tau[2] / tau[1] \n}\n\"\n\n\ndata_list &lt;- list(\n  n = c(19, 15),\n  low = c(91, 46, 95, 60, 33, 410, 105, 43, 189, 1097, 54, 178, 114, 137, 233, 101, 25, 70, 357),\n  normal = c(370, 267, 99, 157, 75, 1281, 48, 298, 268, 62, 804, 430, 171, 694, 404),\n  Lf = 50,\n  Nf = 50\n)\n\n\nrun_jags_model &lt;- function(data_list, n.iter = 10000) {\n  jags_model &lt;- jags.model(textConnection(model_string), data = data_list, n.chains = 1, n.adapt = 5000)\n  update(jags_model, n.iter = 5000)\n  results &lt;- coda.samples(jags_model, variable.names = c(\"mu\", \"tau\", \"med\", \"rmed\", \"test\"), n.iter = n.iter)\n  return(results)\n}\n\n# 运行模型\nresults &lt;- run_jags_model(data_list)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 36\n   Unobserved stochastic nodes: 4\n   Total graph size: 54\n\nInitializing model\n\n# 结果总结\nsummary(results)\n\n\nIterations = 10001:20000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n            Mean      SD Naive SE Time-series SE\nmed[1]  108.9977 24.5102 0.245102       0.316528\nmed[2]  226.9479 63.3989 0.633989       0.816293\nmu[1]     4.6669  0.2211 0.002211       0.002836\nmu[2]     5.3875  0.2729 0.002729       0.003485\nrmed      2.1877  0.7986 0.007986       0.010310\ntau[1]    1.1475  0.3721 0.003721       0.003897\ntau[2]    0.9681  0.3575 0.003575       0.003922\ntest[1]   0.9776  0.1480 0.001480       0.001776\ntest[2]   1.0000  0.0000 0.000000       0.000000\n\n2. Quantiles for each variable:\n\n            2.5%      25%      50%     75%   97.5%\nmed[1]   68.4281  91.9259 106.1203 122.658 164.640\nmed[2]  126.3116 183.6873 219.5296 261.522 372.923\nmu[1]     4.2258   4.5210   4.6646   4.809   5.104\nmu[2]     4.8388   5.2132   5.3915   5.567   5.921\nrmed      1.0242   1.6259   2.0676   2.598   4.060\ntau[1]    0.5360   0.8847   1.1113   1.367   1.982\ntau[2]    0.4043   0.7080   0.9219   1.175   1.792\ntest[1]   1.0000   1.0000   1.0000   1.000   1.000\ntest[2]   1.0000   1.0000   1.0000   1.000   1.000\n\n# 绘制结果（可选）\nplot(results)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 选择不同的先验分布\nsensitivity_analysis &lt;- function(mu1_prior, mu2_prior, tau1_prior, tau2_prior) {\n  model_string &lt;- sprintf(\"\n  model {\n    for(i in 1:n[1]) {\n      low[i] ~ dlnorm(mu[1], tau[1])\n    }\n    for(i in 1:n[2]) {\n      normal[i] ~ dlnorm(mu[2], tau[2])\n    }\n\n    mu[1] ~ %s\n    mu[2] ~ %s\n\n    tau[1] ~ %s\n    tau[2] ~ %s\n    \n    med[1] &lt;- exp(mu[1]) \n    med[2] &lt;- exp(mu[2]) \n    rmed &lt;- med[2] / med[1] \n\n    test[1] &lt;- step(med[2] - med[1]) \n    test[2] &lt;- step(Nf - Lf) \n\n    Lf ~ dlnorm(mu[1], tau[1]) \n    Nf ~ dlnorm(mu[2], tau[2]) \n\n    dmu &lt;- mu[2] - mu[1] \n    rtau &lt;- tau[2] / tau[1] \n  }\n  \", mu1_prior, mu2_prior, tau1_prior, tau2_prior)\n\n  jags_model &lt;- jags.model(textConnection(model_string), data = data_list, n.chains = 1, n.adapt = 5000)\n  update(jags_model, n.iter = 5000)\n  results &lt;- coda.samples(jags_model, variable.names = c(\"mu\", \"tau\", \"med\", \"rmed\", \"test\"), n.iter = 10000)\n  return(results)\n}\n\n# 运行敏感性分析示例\nsensitivity_results &lt;- sensitivity_analysis(\"dunif(-10000, 10000)\", \"dunif(-10000, 10000)\", \"dunif(0, 10000)\", \"dunif(0, 10000)\")\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 36\n   Unobserved stochastic nodes: 4\n   Total graph size: 54\n\nInitializing model\n\n# 总结敏感性分析结果\n summary(sensitivity_results)\n\n\nIterations = 10001:20000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n            Mean      SD Naive SE Time-series SE\nmed[1]  108.6348 22.4151 0.224151       0.289638\nmed[2]  226.8488 58.3761 0.583761       0.750814\nmu[1]     4.6672  0.2037 0.002037       0.002613\nmu[2]     5.3922  0.2537 0.002537       0.003261\nrmed      2.1760  0.7262 0.007262       0.009261\ntau[1]    1.2658  0.3806 0.003806       0.005006\ntau[2]    1.1041  0.3745 0.003745       0.005075\ntest[1]   0.9853  0.1204 0.001204       0.001581\ntest[2]   1.0000  0.0000 0.000000       0.000000\n\n2. Quantiles for each variable:\n\n            2.5%      25%     50%     75%   97.5%\nmed[1]   71.1583  93.1652 106.641 121.682 158.511\nmed[2]  132.5750 187.1773 220.053 258.254 361.766\nmu[1]     4.2649   4.5344   4.669   4.801   5.066\nmu[2]     4.8871   5.2321   5.394   5.554   5.891\nrmed      1.0834   1.6729   2.060   2.560   3.909\ntau[1]    0.6343   0.9907   1.224   1.508   2.099\ntau[2]    0.4989   0.8341   1.062   1.332   1.950\ntest[1]   1.0000   1.0000   1.000   1.000   1.000\ntest[2]   1.0000   1.0000   1.000   1.000   1.000\n\nplot(sensitivity_results)",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 2"
    ]
  },
  {
    "objectID": "causal inference hw 1.html#作业1",
    "href": "causal inference hw 1.html#作业1",
    "title": "causal inference hw 1",
    "section": "作业1",
    "text": "作业1",
    "crumbs": [
      "Causal inference",
      "causal inference hw 1"
    ]
  },
  {
    "objectID": "causal inference hw 1.html#作业2",
    "href": "causal inference hw 1.html#作业2",
    "title": "causal inference hw 1",
    "section": "作业2",
    "text": "作业2\n\nrm(list=ls())\nsuppressMessages(library(tidyverse))\n\nload('/Users/hcy/Documents/MyKeyData/300hobby/370professional_class_note/PhD_hw/Brumback FOCI Website Material/Chapter 6/simdr.r.RData')\n\nload('/Users/hcy/Documents/MyKeyData/300hobby/370professional_class_note/PhD_hw/Brumback FOCI Website Material/Chapter 6/whatif2dat.RData')\n\nglimpse(whatif2dat)\n\nRows: 165\nColumns: 11\n$ vl0      &lt;int&gt; 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0…\n$ vlcont0  &lt;int&gt; 20, 20, 61420, 600, 75510, 20, 40, 86200, 20, 20, 20, 20, 657…\n$ artad0   &lt;int&gt; 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, NA, 1, 0, 1, 1, NA, 1,…\n$ vl4      &lt;int&gt; 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1…\n$ vlcont4  &lt;int&gt; 420, 20, 20, 20, 184420, 30, 20, 1480, 20, 20, 100, 20, 59820…\n$ artad4   &lt;int&gt; 1, 1, 1, 1, 0, 0, 1, 1, 1, NA, 0, 1, 1, 1, 0, 1, 0, 1, NA, 1,…\n$ audit0   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1…\n$ T        &lt;int&gt; 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0…\n$ A        &lt;int&gt; 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0…\n$ lvlcont0 &lt;dbl&gt; 2.995732, 2.995732, 11.025491, 6.396930, 11.232020, 2.995732,…\n$ lvlcont4 &lt;dbl&gt; 6.040255, 2.995732, 2.995732, 2.995732, 12.124971, 3.401197, …",
    "crumbs": [
      "Causal inference",
      "causal inference hw 1"
    ]
  },
  {
    "objectID": "survival hw 1.html#km-8.5",
    "href": "survival hw 1.html#km-8.5",
    "title": "survival hw 1",
    "section": "KM 8.5",
    "text": "KM 8.5\nUsing the data set in Exercise 1, using the Breslow method of handling ties,\n\nAnalyze the data by performing a global test of no effect of group as defined in Exercise 8.1(a) on survival. Construct an ANOVA table to summarize estimates of the risk coefficients and the results of the one degree of freedom tests for each covariate in the model.\n\n\nmodel1 &lt;- coxph(Surv(time,delta)~new,data=hodg,ties = 'breslow')\nsummary(model)\n\nCall:\ncoxph(formula = Surv(time, delta) ~ gtype * dtype, data = hodg, \n    ties = \"breslow\")\n\n  n= 43, number of events= 26 \n\n                coef exp(coef) se(coef)      z Pr(&gt;|z|)   \ngtype        3.00377  20.16134  1.30504  2.302  0.02135 * \ndtype        4.16964  64.69233  1.45172  2.872  0.00408 **\ngtype:dtype -2.33990   0.09634  0.85168 -2.747  0.00601 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\ngtype        20.16134    0.04960   1.56196  260.2361\ndtype        64.69233    0.01546   3.75963 1113.1660\ngtype:dtype   0.09634   10.38019   0.01815    0.5114\n\nConcordance= 0.605  (se = 0.061 )\nLikelihood ratio test= 7.89  on 3 df,   p=0.05\nWald test            = 9.26  on 3 df,   p=0.03\nScore (logrank) test = 11.08  on 3 df,   p=0.01\n\nmodel2 &lt;- coxph(Surv(time, delta) ~ 1, \n              ties=\"breslow\", data = hodg)\nanova(model1, model2)\n\nAnalysis of Deviance Table\n Cox model: response is  Surv(time, delta)\n Model 1: ~ new\n Model 2: ~ 1\n   loglik  Chisq Df Pr(&gt;|Chi|)  \n1 -83.350                       \n2 -87.298 7.8942  3    0.04825 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# anova(model1)\n\n\nRepeat part (a) using the coding as described in Exercise 8.1(b). Furthermore, test the hypothesis of disease type by transplant interaction using a likelihood ratio rest based on this coding. Repeat using the Wald test.\n\n\nmodel3 &lt;- coxph(Surv(time,delta)~gtype * dtype,data=hodg,ties = 'breslow')\nsummary(model)\n\nCall:\ncoxph(formula = Surv(time, delta) ~ gtype * dtype, data = hodg, \n    ties = \"breslow\")\n\n  n= 43, number of events= 26 \n\n                coef exp(coef) se(coef)      z Pr(&gt;|z|)   \ngtype        3.00377  20.16134  1.30504  2.302  0.02135 * \ndtype        4.16964  64.69233  1.45172  2.872  0.00408 **\ngtype:dtype -2.33990   0.09634  0.85168 -2.747  0.00601 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\ngtype        20.16134    0.04960   1.56196  260.2361\ndtype        64.69233    0.01546   3.75963 1113.1660\ngtype:dtype   0.09634   10.38019   0.01815    0.5114\n\nConcordance= 0.605  (se = 0.061 )\nLikelihood ratio test= 7.89  on 3 df,   p=0.05\nWald test            = 9.26  on 3 df,   p=0.03\nScore (logrank) test = 11.08  on 3 df,   p=0.01\n\nanova(model2, model3)\n\nAnalysis of Deviance Table\n Cox model: response is  Surv(time, delta)\n Model 1: ~ 1\n Model 2: ~ gtype * dtype\n   loglik  Chisq Df Pr(&gt;|Chi|)  \n1 -87.298                       \n2 -83.350 7.8942  3    0.04825 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nFind point estimates and 95% confidence intervals for the relative risk of death for an NHL Auto transplant patient as compared to an NHL Allo transplant patient.\n\n\ncoef(model1)[2]\n\nnewautlo_nhl \n    0.663868 \n\nconfint(model1)[2, ]\n\n    2.5 %    97.5 % \n-0.442074  1.769810 \n\n\n\nFind the p-value of a test of the hypothesis that the hazard rates are the same for HOD Allo transplants and NHL Allo patients, using the Wald test. Repeat a similar test for Auto patients.\n\n\nlibrary(aod) # wald.test\n\n\nAttaching package: 'aod'\n\n\nThe following object is masked from 'package:survival':\n\n    rats\n\nwald.test(b = coef(model3), Sigma = vcov(model3), Terms = 1)\n\nWald test:\n----------\n\nChi-squared test:\nX2 = 5.3, df = 1, P(&gt; X2) = 0.021\n\n\n\nTest the hypothesis, using the Wald test, that the hazard rates for Auto transplant and Allo transplant patients are the same for each disease group against the alternative that the hazard rates for Auto transplant and Allo transplant patients for at least one group are different using a two-degree of freedom test of H0 : h(t | NHL Allo) \u0003 h(t | NHL Auto) and H0 : h(t | HOD Allo) \u0003 h(t | HOD Auto).\n\n\nlibrary(aod) # wald.test\nwald.test(b = coef(model3), Sigma = vcov(model3), Terms = 2:3)\n\nWald test:\n----------\n\nChi-squared test:\nX2 = 8.3, df = 2, P(&gt; X2) = 0.016\n\nws.e &lt;- t(coef(model3)[2:3]-0) %*% solve(vcov(model3)[2:3,2:3]) %*% (coef(model3)[2:3]-0)\npchisq(q = ws.e, df = 2, lower.tail = F)\n\n           [,1]\n[1,] 0.01614822",
    "crumbs": [
      "Survival",
      "survival hw 1"
    ]
  },
  {
    "objectID": "survival hw 1.html#km-8.1",
    "href": "survival hw 1.html#km-8.1",
    "title": "survival hw 1",
    "section": "",
    "text": "\\[\\begin{align}\nh(t|NHL Allo) &= h_0(t) \\\\\nh(t|HOD Allo) &= h_0(t)exp(2) \\\\\nh(t|NHL Auto) &= h_0(t)exp(1.5) \\\\\nh(t|HOD Auto) &= h_0(t)exp(0.5) \\\\\n\\end{align}\\]",
    "crumbs": [
      "Survival",
      "survival hw 1"
    ]
  },
  {
    "objectID": "survival hw 2.html",
    "href": "survival hw 2.html",
    "title": "survival hw 2",
    "section": "",
    "text": "a\n\nlibrary(survival)\nlibrary(ClinicalTrialSummary)\ndata(ggas)\n# group 0: Chemotherapy only; 1: Chemotherapy plus radiotherapy\ndat &lt;- ggas\ndat$time &lt;- dat$time * 365\ndat$therapy &lt;- ifelse(dat$group == 0, 1, 0)\n\nmodel_a &lt;- coxph(Surv(time, event) ~ therapy, dat)\nsummary(model_a)\n\nCall:\ncoxph(formula = Surv(time, event) ~ therapy, data = dat)\n\n  n= 90, number of events= 82 \n\n           coef exp(coef) se(coef)      z Pr(&gt;|z|)\ntherapy -0.1051    0.9002   0.2233 -0.471    0.638\n\n        exp(coef) exp(-coef) lower .95 upper .95\ntherapy    0.9002      1.111    0.5811     1.395\n\nConcordance= 0.562  (se = 0.031 )\nLikelihood ratio test= 0.22  on 1 df,   p=0.6\nWald test            = 0.22  on 1 df,   p=0.6\nScore (logrank) test = 0.22  on 1 df,   p=0.6\n\n\nb\n\nmodel_b &lt;- coxph(Surv(time, event) ~ therapy + tt(therapy), dat,\n               tt = function(x, t, ...)x*log(t))\nsummary(model_b)\n\nCall:\ncoxph(formula = Surv(time, event) ~ therapy + tt(therapy), data = dat, \n    tt = function(x, t, ...) x * log(t))\n\n  n= 90, number of events= 82 \n\n                coef exp(coef) se(coef)      z Pr(&gt;|z|)   \ntherapy     -3.86619   0.02094  1.47707 -2.617  0.00886 **\ntt(therapy)  0.64677   1.90935  0.24793  2.609  0.00909 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\ntherapy       0.02094    47.7602  0.001158    0.3786\ntt(therapy)   1.90935     0.5237  1.174488    3.1040\n\nConcordance= 0.576  (se = 0.03 )\nLikelihood ratio test= 8.54  on 2 df,   p=0.01\nWald test            = 6.87  on 2 df,   p=0.03\nScore (logrank) test = 7.59  on 2 df,   p=0.02\n\n\nc\n\ntao &lt;- unique(sort(dat$time[dat$event==1]))\nl &lt;- numeric(length(tao))\nfor (i in 1:length(tao)) {\n  temp &lt;- survSplit(Surv(time, event) ~ group, data= dat, cut=c(tao[i]),\n                    episode= \"tgroup\", id=\"id\")\n  fit &lt;- coxph(Surv(tstart, time, event) ~ group +\n                 group:factor(tgroup), data=temp)\n  l[i] &lt;- logLik(fit)\n}\n\nWarning in agreg.fit(X, Y, istrat, offset, init, control, weights = weights, :\nLoglik converged before variable 1,2 ; beta may be infinite.\n\n\nWarning in agreg.fit(X, Y, istrat, offset, init, control, weights = weights, :\nLoglik converged before variable 2 ; beta may be infinite.\n\n(taoHat &lt;- tao[which.max(l)])\n\n[1] 254\n\n\nd\n\ntemp_d &lt;- survSplit(Surv(time, event) ~ group, data= dat, cut=254,\n                  episode= \"tgroup\", id=\"id\")\ntemp_d$group1 &lt;- ifelse(temp_d$group==0 & temp_d$tgroup==1,1,0)\ntemp_d$group2 &lt;- ifelse(temp_d$group==0 & temp_d$tgroup==2,1,0)\nfit_d2 &lt;- coxph(Surv(tstart, time, event) ~ group1 + group2, data=temp_d)\nsummary(fit_d2)\n\nCall:\ncoxph(formula = Surv(tstart, time, event) ~ group1 + group2, \n    data = temp_d)\n\n  n= 150, number of events= 82 \n\n          coef exp(coef) se(coef)      z Pr(&gt;|z|)   \ngroup1 -1.4229    0.2410   0.4327 -3.288  0.00101 **\ngroup2  0.6423    1.9008   0.3048  2.107  0.03508 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n       exp(coef) exp(-coef) lower .95 upper .95\ngroup1     0.241     4.1491    0.1032    0.5628\ngroup2     1.901     0.5261    1.0460    3.4542\n\nConcordance= 0.62  (se = 0.029 )\nLikelihood ratio test= 17.79  on 2 df,   p=1e-04\nWald test            = 15.25  on 2 df,   p=5e-04\nScore (logrank) test = 17.31  on 2 df,   p=2e-04",
    "crumbs": [
      "Survival",
      "survival hw 2"
    ]
  },
  {
    "objectID": "bayesian hw 3.html",
    "href": "bayesian hw 3.html",
    "title": "bayesian hw 3",
    "section": "",
    "text": "第一题",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 3"
    ]
  },
  {
    "objectID": "bayesian hw 3.html#第二题",
    "href": "bayesian hw 3.html#第二题",
    "title": "bayesian hw 3",
    "section": "第二题",
    "text": "第二题",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 3"
    ]
  },
  {
    "objectID": "bayesian hw 3.html#第三题",
    "href": "bayesian hw 3.html#第三题",
    "title": "bayesian hw 3",
    "section": "第三题",
    "text": "第三题",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 3"
    ]
  },
  {
    "objectID": "bayesian hw 3.html#a",
    "href": "bayesian hw 3.html#a",
    "title": "bayesian hw 3",
    "section": "a",
    "text": "a\n临床试验的例子，\n安慰剂服从\\(Y～N(\\mu,\\sigma^2) ,i=1,..,n\\) ,\n实验药服从\\(N～N(\\mu + \\delta,\\sigma^2),i=1+n,..,n+m\\)",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 3"
    ]
  },
  {
    "objectID": "bayesian hw 3.html#b",
    "href": "bayesian hw 3.html#b",
    "title": "bayesian hw 3",
    "section": "b",
    "text": "b",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 3"
    ]
  },
  {
    "objectID": "bayesian hw 3.html#c",
    "href": "bayesian hw 3.html#c",
    "title": "bayesian hw 3",
    "section": "c",
    "text": "c\n\n# 先验参数\nn &lt;- m &lt;- 50\nmu &lt;- 10\ndelta &lt;- 1\nsigma &lt;- 2\n\n# 模拟数据\ny1 &lt;- rnorm(n,mu,sigma)\ny2 &lt;- rnorm(m,mu+delta,sigma)\ny &lt;- c(y1,y2)\n\n# Gibbs\nset.seed(2024)\nS&lt;-1000\nPHI&lt;-matrix(nrow=S,ncol=3)\nPHI[1,]&lt;-phi&lt;-c(0, 0, 4) # sigma2\n\n## Gibbs sampling algorithm\nfor(s in 2:S) {\n\n# generate a new mu value from its full conditional\nmu_n&lt;- n*100*100*mean(y1)/(4+n*100*100)\ns2_n&lt;- (100*100*4)/(4+n*100*100)\nphi[1]&lt;-rnorm(1, mu_n, sqrt(s2_n) )\n\n# generate a new delta value from its full conditional\ndelta_m&lt;-  ( m*100*100*mean(y2)/(4+m*100*100))-( n*100*100*mean(y1)/(4+n*100*100))\ns2_m&lt;- (100*100*4)/(4+m*100*100)\nphi[2]&lt;-rnorm(1, delta_m, sqrt(s2_m) )\n\n# generate a new sigma^2 value from its full conditional\na &lt;- 0.01+(m+n)*0.5\nb &lt;-0.01+(m+n)*0.5*((sum((y-mean(y))^2)/(m+n))+((mean(y)-(n*mu+m*(mu+delta))/(m+n))^2))\nphi[3]&lt;- 1/rgamma(1, a, b)\n\nPHI[s,]&lt;-phi         }\n\n\nplot(density(PHI[,1]),main='mu')\n\n\n\n\n\n\n\nplot(density(PHI[,2]),main='delta')\n\n\n\n\n\n\n\nplot(density(PHI[,3]),main='sigma2')",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 3"
    ]
  },
  {
    "objectID": "bayesian hw 3.html#d",
    "href": "bayesian hw 3.html#d",
    "title": "bayesian hw 3",
    "section": "d",
    "text": "d\n\nlibrary('rjags')\n\nLoading required package: coda\n\n\nLinked to JAGS 4.3.2\n\n\nLoaded modules: basemod,bugs\n\nlibrary(\"coda\")\n\n\nmodel_code &lt;- \"\nmodel {\n\n  for (i in 1:n) {\n    Y[i] ~ dnorm(mu, 1/sigma2)\n  }\n  \n  for (i in (n+1):(n+m)) {\n    Y[i] ~ dnorm(mu + delta, 1/sigma2)\n  }\n\n  mu ~ dnorm(0, 1/10000)\n  delta ~ dnorm(0, 1/10000)\n  tau ~ dgamma(0.01, 0.01)\n  sigma2 = 1/tau\n}\n\"\n\nn &lt;- 50\nm &lt;- 50\nmu_n &lt;- 10\ndelta &lt;- 1\nsigma &lt;- 2\n\nset.seed(2024)\nY &lt;- c(rnorm(n, mu_n, sigma), rnorm(m, mu_n + delta, sigma))\n\ndata_list &lt;- list(Y = Y, n = n, m = m)\n\n\n\nmodel &lt;- jags.model(textConnection(model_code), data = data_list, n.chains = 3, n.adapt = 1000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 100\n   Unobserved stochastic nodes: 3\n   Total graph size: 113\n\nInitializing model\n\n# 运行 MCMC 采样\nupdate(model, 1000) # Burn-in阶段\nsamples &lt;- coda.samples(model, variable.names = c(\"mu\", \"delta\", \"sigma2\"), n.iter = 5000)\n\nplot(samples)",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 3"
    ]
  },
  {
    "objectID": "bayesian hw 3.html#e",
    "href": "bayesian hw 3.html#e",
    "title": "bayesian hw 3",
    "section": "e",
    "text": "e\n\ngelman.diag(samples)\n\nPotential scale reduction factors:\n\n       Point est. Upper C.I.\ndelta           1          1\nmu              1          1\nsigma2          1          1\n\nMultivariate psrf\n\n1\n\n\n\n# 自相关\nacf(as.matrix(samples)[, \"mu\"])\n\n\n\n\n\n\n\nacf(as.matrix(samples)[, \"delta\"])\n\n\n\n\n\n\n\nacf(as.matrix(samples)[, \"sigma2\"])\n\n\n\n\n\n\n\n\n\n# 有效样本量\neffectiveSize(samples)\n\n   delta       mu   sigma2 \n5024.495 4852.328 9014.855",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 3"
    ]
  },
  {
    "objectID": "bayesian hw 3.html#a-1",
    "href": "bayesian hw 3.html#a-1",
    "title": "bayesian hw 3",
    "section": "a",
    "text": "a\nbeta分布对于概率\\(\\theta\\)来说是常规的先验分布设置手段。因为其足够灵活并且在0，1之间变化; 其次，这样设置还有现实意义的解释，即beta分布中，a=命中多少个，b=未能命中的个数; 最后，添加了一个exp(m)这样一个调节的量，表示在常规命中率的基础上引入变化\n\nset.seed(2024)\ntheta_i &lt;- c(0.845, 0.847, 0.880, 0.674, 0.909, 0.898, 0.770, 0.801, 0.802, 0.875)\nm_samples &lt;- rnorm(1000, mean = 0, sd = 10)\n\nprobabilities &lt;- numeric(10000)\n\nfor (i in 1:1000) {\n  m &lt;- m_samples[i] \n\n  for (j in 1:10) {\n    shape1 &lt;- exp(m) * theta_i[j]       \n    shape2 &lt;- exp(m) * (1 - theta_i[j]) \n    probability &lt;- rbeta(1, shape1, shape2)\n    probabilities[(i - 1) * 10 + j] &lt;- probability\n  }\n}\n\n\nhist(probabilities,probability = TRUE)\nlines(density(probabilities), col = \"red\", lwd = 2)",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 3"
    ]
  },
  {
    "objectID": "bayesian hw 3.html#b-1",
    "href": "bayesian hw 3.html#b-1",
    "title": "bayesian hw 3",
    "section": "b",
    "text": "b\nm引入了不确定性",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 3"
    ]
  },
  {
    "objectID": "bayesian hw 3.html#c-1",
    "href": "bayesian hw 3.html#c-1",
    "title": "bayesian hw 3",
    "section": "c",
    "text": "c\n后验： $$\nbeta(y+exp(m)q_{i} , exp(m)(1-q_i)+n-y)\n$$",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 3"
    ]
  },
  {
    "objectID": "bayesian hw 3.html#d-1",
    "href": "bayesian hw 3.html#d-1",
    "title": "bayesian hw 3",
    "section": "d",
    "text": "d\n\ntheta_i &lt;- c(0.845, 0.847, 0.880, 0.674, 0.909, 0.898, 0.770, 0.801, 0.802, 0.875)\nclutch_makes &lt;- c(64, 72, 55, 27, 75, 24, 28, 66, 40, 13)\nclutch_attempts &lt;- c(75, 95, 63, 39, 83, 26, 41, 82, 54, 16)\nn_players &lt;- length(theta_i)\n\nset.seed(2024)\nS&lt;-1000000\nTHETA&lt;-matrix(nrow=S,ncol=11)\nTHETA[1,]&lt;-theta&lt;-c(0,rep(0.8,10)) # theta\n\n\nfor(s in 2:S) {\n\ntheta[1]&lt;- m &lt;- rnorm(1, 0, sqrt(10))\n\nfor(i in 1:10){\n\ntheta[i+1] &lt;- rbeta(1,\n                  clutch_makes[i]+exp(m)*theta_i[i],\n                  exp(m)*(1-theta_i[i])+clutch_attempts[i]-clutch_makes[i])\n}\n\nTHETA[s,]&lt;-theta   }\n\ncolnames(THETA) &lt;- c('m',paste0('theta',1:10))\nm  &lt;- apply(THETA,2,function(x){\n  quantile(x,0.5)\n})\nl &lt;- apply(THETA,2,function(x){\n  quantile(x,0.025)\n})\nh &lt;- apply(THETA,2,function(x){\n  quantile(x,0.975)\n})\n\nt(rbind(l,m,h))\n\n                 l           m         h\nm       -6.1908236 0.001690084 6.2067146\ntheta1   0.7694963 0.854152849 0.9206071\ntheta2   0.6741323 0.768909606 0.8476116\ntheta3   0.7863799 0.877737520 0.9402216\ntheta4   0.5491007 0.689603429 0.8171368\ntheta5   0.8359270 0.907343843 0.9552716\ntheta6   0.8051724 0.924314204 0.9876626\ntheta7   0.5476745 0.702025546 0.8137787\ntheta8   0.7169512 0.805951434 0.8797382\ntheta9   0.6270672 0.754245031 0.8463356\ntheta10  0.6228891 0.843445009 0.9521655",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 3"
    ]
  },
  {
    "objectID": "bayesian hw 3.html#e-1",
    "href": "bayesian hw 3.html#e-1",
    "title": "bayesian hw 3",
    "section": "e",
    "text": "e\n\nlibrary(rjags)\nlibrary(coda)\n\nmodel_string &lt;- \"\nmodel {\n  for (i in 1:n_players) {\n\n    clutch_makes[i] ~ dbin(theta[i], clutch_attempts[i])\n\n    theta[i] ~ dbeta(alpha[i], beta[i])\n\n    alpha[i] &lt;- exp(m[i]) * theta_i[i]\n    beta[i] &lt;- exp(m[i]) * (1 - theta_i[i]) \n  }\n}\n\"\n\ntheta_i &lt;- c(0.845, 0.847, 0.880, 0.674, 0.909, 0.898, 0.770, 0.801, 0.802, 0.875)\nclutch_makes &lt;- c(64, 72, 55, 27, 75, 24, 28, 66, 40, 13)\nclutch_attempts &lt;- c(75, 95, 63, 39, 83, 26, 41, 82, 54, 16)\nn_players &lt;- length(theta_i)\nm &lt;- rnorm(n_players*10,0,sqrt(10))\n\n\ndata_jags &lt;- list(\n  theta_i = theta_i,         \n  clutch_makes = clutch_makes,\n  clutch_attempts = clutch_attempts,\n  n_players = n_players,\n  m=m\n)\n\n\n\nmodel &lt;- jags.model(textConnection(model_string), data = data_jags,  n.chains = 3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 10\n   Unobserved stochastic nodes: 10\n   Total graph size: 182\n\nInitializing model\n\nupdate(model, 10000)\n\nn_samples &lt;- 10000\nsamples &lt;- coda.samples(model, variable.names = c('theta'), n.iter = n_samples)\n\nplot(samples)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsummary(samples)\n\n\nIterations = 11001:21000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n            Mean      SD  Naive SE Time-series SE\ntheta[1]  0.8532 0.04063 0.0002346      0.0003065\ntheta[2]  0.7605 0.04285 0.0002474      0.0003274\ntheta[3]  0.8725 0.04190 0.0002419      0.0003270\ntheta[4]  0.6906 0.07193 0.0004153      0.0005359\ntheta[5]  0.9039 0.03185 0.0001839      0.0002457\ntheta[6]  0.9229 0.05088 0.0002938      0.0004945\ntheta[7]  0.7330 0.04478 0.0002585      0.0003429\ntheta[8]  0.8046 0.04287 0.0002475      0.0003218\ntheta[9]  0.7413 0.05935 0.0003427      0.0004421\ntheta[10] 0.8186 0.08916 0.0005148      0.0007476\n\n2. Quantiles for each variable:\n\n            2.5%    25%    50%    75%  97.5%\ntheta[1]  0.7655 0.8275 0.8559 0.8823 0.9235\ntheta[2]  0.6720 0.7321 0.7624 0.7903 0.8389\ntheta[3]  0.7802 0.8468 0.8765 0.9028 0.9426\ntheta[4]  0.5411 0.6435 0.6934 0.7415 0.8207\ntheta[5]  0.8343 0.8841 0.9073 0.9271 0.9569\ntheta[6]  0.7960 0.8966 0.9335 0.9606 0.9896\ntheta[7]  0.6403 0.7035 0.7348 0.7640 0.8162\ntheta[8]  0.7126 0.7774 0.8073 0.8347 0.8814\ntheta[9]  0.6162 0.7032 0.7442 0.7826 0.8478\ntheta[10] 0.6135 0.7652 0.8309 0.8851 0.9552",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 3"
    ]
  },
  {
    "objectID": "bayesian hw 3.html#f",
    "href": "bayesian hw 3.html#f",
    "title": "bayesian hw 3",
    "section": "f",
    "text": "f\n自己的代码需要推导后验分布",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 3"
    ]
  },
  {
    "objectID": "bayesian hw 3.html#g",
    "href": "bayesian hw 3.html#g",
    "title": "bayesian hw 3",
    "section": "g",
    "text": "g\n\ngelman.diag(samples)\n\nPotential scale reduction factors:\n\n          Point est. Upper C.I.\ntheta[1]           1          1\ntheta[2]           1          1\ntheta[3]           1          1\ntheta[4]           1          1\ntheta[5]           1          1\ntheta[6]           1          1\ntheta[7]           1          1\ntheta[8]           1          1\ntheta[9]           1          1\ntheta[10]          1          1\n\nMultivariate psrf\n\n1\n\n\n\neffectiveSize(samples)\n\n theta[1]  theta[2]  theta[3]  theta[4]  theta[5]  theta[6]  theta[7]  theta[8] \n 17583.03  17296.29  16485.81  18027.44  16844.73  10661.30  17111.56  17745.82 \n theta[9] theta[10] \n 18041.72  14355.20",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 3"
    ]
  },
  {
    "objectID": "bayesian hw 3.html#a-2",
    "href": "bayesian hw 3.html#a-2",
    "title": "bayesian hw 3",
    "section": "a",
    "text": "a\n\nlibrary(MASS)\ndata(galaxies)\n?galaxies\nY &lt;- galaxies\n\nn &lt;- length(Y)\n\nhist(Y,breaks=25)\n\n\n\n\n\n\n\n\n\\(\\mu=20000\\)\n\\(\\sigma=5000\\)\n\\(k=15\\)",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 3"
    ]
  },
  {
    "objectID": "bayesian hw 3.html#b-2",
    "href": "bayesian hw 3.html#b-2",
    "title": "bayesian hw 3",
    "section": "b",
    "text": "b\n\nlibrary(rjags)\nlibrary(coda)\n\nmodel_string &lt;- \"\nmodel {\n  for (i in 1:s) {\n    Y[i] ~ dt(mu, tau, k)\n  }\n  mu ~ dnorm(0, 1/10000^2)\n  tau ~ dgamma(0.01, 0.01)\n  k ~ dunif(1, 30)\n}\n\"\n\ndata_jags &lt;- list(Y =galaxies, s = length(galaxies))\n\n# 设置初始值\ninit_values &lt;- list(\n  list(mu = 0, tau = 100, k = 15)\n)\n\n# 创建模型\nmodel &lt;- jags.model(textConnection(model_string), data = data_jags, inits = init_values, n.chains = 1)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 82\n   Unobserved stochastic nodes: 3\n   Total graph size: 94\n\nInitializing model\n\n# 更新模型\nupdate(model, 10000)\n\n# 采样\nn_samples &lt;- 10000\nsamples &lt;- coda.samples(model, variable.names = c(\"mu\", \"tau\", \"k\"), n.iter = n_samples)\n\n# 绘制轨迹图\nplot(samples)",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 3"
    ]
  },
  {
    "objectID": "bayesian hw 3.html#c-2",
    "href": "bayesian hw 3.html#c-2",
    "title": "bayesian hw 3",
    "section": "c",
    "text": "c\n\nposterior_means &lt;- apply(as.matrix(samples), 2, mean)\n\n# 从后验均值计算t分布\nmu_posterior &lt;- posterior_means[\"mu\"]\nsigma_posterior &lt;- 1/sqrt(posterior_means[\"tau\"])\nk_posterior &lt;- posterior_means[\"k\"]\n\n# 生成t分布的随机样本\nt_dist_samples &lt;- rt(10000, df = k_posterior, ncp = mu_posterior)\n\n# 绘制观察数据和t分布的对比图\nhist(galaxies, probability = TRUE, col = rgb(0, 0, 1, 0.5), main = \"Comparison of Data and t-distribution\", xlim = c(min(galaxies), max(galaxies)))\nlines(density(t_dist_samples), col = \"red\", lwd = 2,xlim=c(10000,35000))",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 3"
    ]
  },
  {
    "objectID": "bayesian hw 4.html",
    "href": "bayesian hw 4.html",
    "title": "bayesian hw 4",
    "section": "",
    "text": "2.4",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 4"
    ]
  },
  {
    "objectID": "bayesian hw 4.html#a",
    "href": "bayesian hw 4.html#a",
    "title": "bayesian hw 4",
    "section": "a",
    "text": "a\n\nset.seed(2024)\nn &lt;- 50\np &lt;- 0.4\nsigma2 &lt;- p*(1-p)/n\n\np1 &lt;- rnorm(10000,p,sqrt(sigma2))\n\nlogit_trans &lt;- function(x){log(x/(1-x))}\ninverse_logit &lt;- function(x){exp(x)/(1+exp(x))}\n\n\nlogit_p1 &lt;- logit_trans(p1)\n\nlogit_p_mean &lt;- logit_trans(p)\nlogit_p_sd &lt;- sd(logit_p1)\n\nlogCI &lt;- c((logit_p_mean+qnorm(0.025)*logit_p_sd),\n           logit_p_mean,\n           (logit_p_mean+qnorm(0.975)*logit_p_sd)\n           )\n\nCI &lt;- inverse_logit(logCI)\nCI\n\n[1] 0.2716469 0.4000000 0.5437262",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 4"
    ]
  },
  {
    "objectID": "bayesian hw 4.html#b",
    "href": "bayesian hw 4.html#b",
    "title": "bayesian hw 4",
    "section": "b",
    "text": "b\n\nset.seed(2024)\nn &lt;- 20\np &lt;- 8/20\nsigma2 &lt;- p*(1-p)/n\n\np1 &lt;- rnorm(10000,p,sqrt(sigma2))\n\nlogit_trans &lt;- function(x){log(x/(1-x))}\ninverse_logit &lt;- function(x){exp(x)/(1+exp(x))}\n\n\nlogit_p1 &lt;- logit_trans(p1)\n\nlogit_p_mean &lt;- logit_trans(p)\nlogit_p_sd &lt;- sd(logit_p1)\n\nlogCI &lt;- c((logit_p_mean+qnorm(0.025)*logit_p_sd),\n           logit_p_mean,\n           (logit_p_mean+qnorm(0.975)*logit_p_sd)\n           )\n\nCI &lt;- inverse_logit(logCI)\nCI\n\n[1] 0.2013291 0.4000000 0.6380887",
    "crumbs": [
      "Bayesian",
      "Bayesian hw 4"
    ]
  },
  {
    "objectID": "causal inference hw 2.html",
    "href": "causal inference hw 2.html",
    "title": "causal inference hw 2",
    "section": "",
    "text": "高阶",
    "crumbs": [
      "Causal inference",
      "causal inference hw 2"
    ]
  },
  {
    "objectID": "causal inference hw 2.html#第二章",
    "href": "causal inference hw 2.html#第二章",
    "title": "causal inference hw 2",
    "section": "第二章",
    "text": "第二章\n\n\n\n-((dnorm(Inf)-dnorm(0.5))/(pnorm(Inf)-pnorm(0.5)))-(-((dnorm(0.5)-dnorm(-Inf))/(pnorm(0.5)-pnorm(-Inf))))+(-0.5)\n\n[1] 1.150238\n\n\n\n\n\nset.seed(2024)\nnsim &lt;- 10 #只是为了多做几次结果看图\nouts &lt;- matrix(NA,nsim,3)\n\nfor(i in 1:nsim){\n  Y1 &lt;- rnorm(10,1,1)\n  Y0 &lt;- rnorm(10,-1,1)\n  delta1 &lt;- median(Y1)-median(Y0)\n  delta2 &lt;- median(Y1-Y0)\n  EY &lt;- mean(Y1-Y0)\n  out &lt;- c(delta1,delta2,EY )\n  outs[i,] &lt;- out\n}\nouts &lt;- data.frame(delta1=outs[,1],delta2=outs[,2],EY=outs[,3])\n\nplot(density(outs$EY))\nlines(density(outs$delta1),col ='red')\nlines(density(outs$delta2),col ='blue')\n\n\n\n\n\n\n\n\n模拟可以看到当Y的样本量较小时，delta2 更贴近平均因果效应 Y的样本量较大时，delta1和dalta2无明显差异（改代码自己跑）\n但是MC多次后delta1和delta2 差别不大了",
    "crumbs": [
      "Causal inference",
      "causal inference hw 2"
    ]
  },
  {
    "objectID": "causal inference hw 2.html#第三章",
    "href": "causal inference hw 2.html#第三章",
    "title": "causal inference hw 2",
    "section": "第三章",
    "text": "第三章\n\n\n\n\n\n\n\nlibrary(Matching)\n\nLoading required package: MASS\n\n\n## \n##  Matching (Version 4.10-15, Build Date: 2024-10-14)\n##  See https://www.jsekhon.com for additional documentation.\n##  Please cite software as:\n##   Jasjeet S. Sekhon. 2011. ``Multivariate and Propensity Score Matching\n##   Software with Automated Balance Optimization: The Matching package for R.''\n##   Journal of Statistical Software, 42(7): 1-52. \n##\n\ndata(lalonde)\n# z = lalonde$treat\n# y = lalonde$re78\n\nmodel &lt;- lm(re78~.,lalonde)\n#model$residuals\n\ntauhat = t.test(model$residuals[z == 1], model$residuals[z == 0], \n                var.equal = TRUE)$p.value\ntauhat\n\n[1] 0.105625\n\nstudent = t.test(y[z == 1], y[z == 0],\n                 var.equal = FALSE)$p.value\nstudent\n\n[1] 0\n\nW = wilcox.test(y[z == 1], y[z == 0])$p.value\nW\n\n[1] 0\n\nD = ks.test(y[z == 1], y[z == 0])$p.value\n\nWarning in ks.test.default(y[z == 1], y[z == 0]): p-value will be approximate\nin the presence of ties\n\nD\n\n[1] 0\n\nsummary(model)\n\n\nCall:\nlm(formula = re78 ~ ., data = lalonde)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -9612  -4355  -1572   3054  53119 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  2.567e+02  3.522e+03   0.073  0.94193   \nage          5.357e+01  4.581e+01   1.170  0.24284   \neduc         4.008e+02  2.288e+02   1.751  0.08058 . \nblack       -2.037e+03  1.174e+03  -1.736  0.08331 . \nhisp         4.258e+02  1.565e+03   0.272  0.78562   \nmarried     -1.463e+02  8.823e+02  -0.166  0.86835   \nnodegr      -1.518e+01  1.006e+03  -0.015  0.98797   \nre74         1.234e-01  8.784e-02   1.405  0.16079   \nre75         1.974e-02  1.503e-01   0.131  0.89554   \nu74          1.380e+03  1.188e+03   1.162  0.24590   \nu75         -1.071e+03  1.025e+03  -1.045  0.29651   \ntreat        1.671e+03  6.411e+02   2.606  0.00948 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6517 on 433 degrees of freedom\nMultiple R-squared:  0.05822,   Adjusted R-squared:  0.0343 \nF-statistic: 2.433 on 11 and 433 DF,  p-value: 0.005974",
    "crumbs": [
      "Causal inference",
      "causal inference hw 2"
    ]
  },
  {
    "objectID": "causal inference hw 2.html#第一章",
    "href": "causal inference hw 2.html#第一章",
    "title": "causal inference hw 2",
    "section": "第一章",
    "text": "第一章\n\n\nz &lt;- rbinom(10000,1,0.5)\ny &lt;- rbinom(10000,1,0.3)\n\nt &lt;- table(z,y)\n\nrd &lt;- function(x){x[1,1]/(x[1,2]+x[1,1])-x[2,1]/(x[2,2]+x[2,1])}\nrr &lt;- function(x){(x[1,1]/(x[1,2]+x[1,1]))/(x[2,1]/(x[2,2]+x[2,1]))}\nor &lt;- function(x){(x[1,1]*x[2,2])/(x[2,1]*x[1,2]) }\n\nrd(t)\n\n[1] -0.002015582\n\nrr(t)\n\n[1] 0.9971382\n\nor(t)\n\n[1] 0.9903871\n\nz &lt;- rbinom(10000,1,0.5)\ny &lt;- rbinom(10000,1,z*0.5+0.1)\n\nt &lt;- table(z,y)\n\nrd &lt;- function(x){x[1,1]/(x[1,2]+x[1,1])-x[2,1]/(x[2,2]+x[2,1])}\nrr &lt;- function(x){(x[1,1]/(x[1,2]+x[1,1]))/(x[2,1]/(x[2,2]+x[2,1]))}\nor &lt;- function(x){(x[1,1]*x[2,2])/(x[2,1]*x[1,2]) }\n\nrd(t)\n\n[1] 0.5151879\n\nrr(t)\n\n[1] 2.31089\n\nor(t)\n\n[1] 15.27893",
    "crumbs": [
      "Causal inference",
      "causal inference hw 2"
    ]
  }
]